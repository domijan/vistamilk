---
title: "Vista Milk: predictions from an ensemble model"
author: Katarina Domijan
date: April 29, 2021
output: beamer_presentation
editor_options: 
  chunk_output_type: console
---


## Data:

```{r echo = FALSE, message=FALSE, warning=FALSE, results = 'hide'}
set.seed(1979)
library(tidyverse)
library(dendextend)
library(glmnet) # for the ridge regression
library(pls)
library(ranger)
library(bartMachine)
library(e1071)
library(BKPC)
library(kernlab)

tr <- read.csv("data/tr.csv") 
te <- read.csv("data/te.csv") 

calcRMSE <- function(y, yhat){sqrt(mean((yhat - y)^2))}


y <- tr[ ,1:3]
tr <- tr[ , -c(1:3)]


a <- apply(tr, 2, mean)
b <- apply(tr, 2, sd)


tr <- scale(tr, a, b)
te <- scale(te, a, b)


kfunc <-  rbfdot(sigma = 0.0005)

```



- The data were scaled and centered

```{r fig.cap="Scaled training data", echo=FALSE}



D <- as.matrix(dist(tr, method="euclidean"))


h <- hclust(as.dist(D), method = "ward.D2")
d <- as.dendrogram(h)

# modify the dendrogram to have some colors in the branches and labels
d2 <- color_branches(d, k=7, col=c(2, 3, 5, 4, 6, 7, 8)) 

par(mfrow = c(1, 2))
plot(d2)

labl.Ave <- cutree(d, 7)
matplot(t(tr), type = "l", col = labl.Ave + 1, lty = 1, ylab = "transmittance values") # see the 7 clusters

dev.off();
```



## Cross - validation set up:

- Split the full training set into 50 random splits of sample size 200 (training) and 353 (validation).
- Fit the models to the randomly selected training (sub)-sets and compare using RMSE of the validation sets.



## Algorithms

Submitted predictions are from an ensemble model -  averages taken over predictions from the following models:


1. pls with 3, 4, and 5 components (R package **pls**)
2. linear regression with 6 PCs with highest eigenvalues
3. lasso (R package **glmnet**)
4. random forest (R package **ranger**) with and without regularization
5. BART (R package **bartMachine**)

## Classifiers cont'd

6. support vector machine for regression (R package **e1071**)
7. linear regression with 6 KPCs with highest eigenvalues using Gaussian kernel (R package **kernlab** and **BKPC**)
8. random forest with 6 KPCs with highest eigenvalues using Gaussian kernel
9. random forest with a Gaussian kernel
10. linear regression with 15 wavelengths selected by a) clustering the wavelengths into 15 clusters and selecting the wavelength from each cluster that has the highest correlation with the response variable


```{r echo=FALSE}
# clustering the columns (variables) rather than observations
D2 <- as.matrix(1 - abs(cor(tr)))

h2<- hclust(as.dist(D2), method = "ward.D2")
dnew <- as.dendrogram(h2)

indexcor <- abs(cor(cbind(y, tr), use = "pairwise.complete.obs"))[1 : 3, -c(1 : 3)]

labl.cols <- cutree(dnew, 15) # 15 variables

# Pick 15 top features
nonz1 <- NULL
nonz2 <- NULL
nonz3 <- NULL
for (j in unique(labl.cols)){
  nonz1[j] <- (which(indexcor[1, ] == max(indexcor[1, labl.cols == j])))
  nonz2[j] <- (which(indexcor[2, ] == max(indexcor[2, labl.cols == j])))
  nonz3[j] <- (which(indexcor[3, ] == max(indexcor[3, labl.cols == j])))
}
```

## kappa_casein

```{r echo = FALSE}
N <- 50
```


```{r, fig.cap="kappa_casein", echo=FALSE, message=FALSE, warning=FALSE}


dat <- cbind((y[,1]), tr)
dat <- as.data.frame(dat)
dat <- dat %>% mutate(class = as.factor(labl.Ave))
dat <- dat %>% na.omit()
nonz <- nonz1

random_split <- replicate(N, sample(nrow(dat), 200))


RMSE <- matrix(0, N, 15)
predictions <- matrix(0, nrow(dat)-200, 14)


########################################################


for (i in 1:N){
  j <- 1
  dat.tr <- dat[random_split[,i],]
  dat.te <- dat[-random_split[,i],]

  ########################################################

  x <- model.matrix(V1 ~. , data = dat.tr[, -1062])

  y1 <- dat.tr[,1]

  grid <- 10^seq(-3, 3, length = 100)

  lasso.fit <- glmnet(x,y1,alpha=1, lambda = grid) # for
  cv.out <- cv.glmnet(x,y1,alpha=1)

  lasso.fit <- glmnet(x,y1,alpha=1,
                      lambda = cv.out$lambda.min)

  lasso.pred <- predict(lasso.fit, newx = model.matrix(V1 ~. , data = dat.te[, -1062]))
  RMSE[i,j] <- calcRMSE(dat.te[,1], lasso.pred)
  predictions[,j] <- lasso.pred
  j <- j + 1


########################################################

  pc <- prcomp(dat.tr[, 2:1061], scale = TRUE)

  lm.fit<- lm(dat.tr[, 1]~pc$x[,1:5])

  x.te <- predict(pc, dat.te[, 2:1061])

  lm.pred <- as.matrix(cbind(rep(1, nrow(x.te)), x.te[,1:5])) %*% as.matrix(lm.fit$coef[1:6])

  RMSE[i,j] <- calcRMSE(dat.te[,1], lm.pred)
  predictions[,j] <- lm.pred
  j <- j + 1

########################################################

  pls.fit <- plsr(V1 ~., data=dat.tr[, -1062], scale= TRUE, validation = "CV")

  pls.pred <- predict(pls.fit, dat.te[,-1], ncomp = 3)

  RMSE[i,j] <- calcRMSE(dat.te[,1], pls.pred)
  predictions[,j] <- pls.pred
  j <- j+ 1

  pls.pred <- predict(pls.fit, dat.te[,-1], ncomp = 4)

  RMSE[i,j] <- calcRMSE(dat.te[,1], pls.pred)
  predictions[,j] <- pls.pred
  j <- j+ 1

  pls.pred <- predict(pls.fit, dat.te[,-1], ncomp = 5)

  RMSE[i,j] <- calcRMSE(dat.te[,1], pls.pred)
  predictions[,j] <- pls.pred
  j <- j+ 1

  ########################################################
  lm.fit <- lm(dat.tr[, 1]~ as.matrix(dat.tr[, nonz+1]) )

  x <- model.matrix(V1 ~. , data = dat.te[, c(1, nonz+1)])

  lm.pred <- x %*% as.matrix(lm.fit$coef)

  RMSE[i,j] <- calcRMSE(dat.te[,1], lm.pred)
  predictions[,j] <- lm.pred
  j <- j+ 1

  ########################################################

  rf.fit <- ranger(V1 ~ ., data = dat.tr, importance = "impurity")

  rf.pred <- predict(rf.fit , data = dat.te)

  RMSE[i,j] <- calcRMSE(dat.te[,1], rf.pred$predictions)
  predictions[,j] <- rf.pred$predictions
  j <- j+1

  ########################################################

  rf.fit <- ranger(V1 ~ ., data = dat.tr, importance = "impurity",regularization.factor = 0.2, regularization.usedepth=FALSE)
  # plot(rf.fit$variable.importance)
  rf.pred <- predict(rf.fit , data = dat.te)

  RMSE[i,j] <- calcRMSE(dat.te[,1], rf.pred$predictions)
  predictions[,j] <- rf.pred$predictions
  j <- j+1

  ########################################################


  bart.fit <- bartMachine(as.data.frame(dat.tr[,-1]), dat.tr[,1], verbose = FALSE)
  # summary(bart.fit)
  bart.pred <- predict(bart.fit, as.data.frame(dat.te[,-1]))
  RMSE[i,j] <- calcRMSE(dat.te[,1], bart.pred)
  predictions[,j] <- bart.pred
  j <-j+1

  ########################################################


  Ktrain2 <- kernelMatrix(kfunc, as.matrix(dat.tr[, 2 : 1061]))
  Ktest2 <- kernelMatrix(kfunc, as.matrix(dat.te[, 2 : 1061]), as.matrix(dat.tr[, 2 : 1061]))
 kpcKern2 <- kPCA(Ktrain2)

  KPCpred3 <- predict(kpcKern2, Ktest2)
  # pairs(KPCpred3[ , 1 : 6], col = dat.te$class)
  lm.fit <- lm(dat.tr[, 1]~  kpcKern2$KPCs[ , 1 : 6] )

  lm.pred <- cbind(rep(1,nrow(KPCpred3)), KPCpred3[ , 1 : 6]) %*% as.matrix(lm.fit$coef)

  RMSE[i,j] <- calcRMSE(dat.te[,1], lm.pred)
  predictions[,j] <- lm.pred
  j <- j + 1

  ########################################################

  rf.fit <- ranger(y = dat.tr[, 1], x =  kpcKern2$KPCs[ , 1 : 6], importance = "impurity")

  rf.pred <- predict(rf.fit , data = KPCpred3[ , 1 : 6])

  RMSE[i,j] <- calcRMSE(dat.te[,1], rf.pred$predictions)
  predictions[,j] <- rf.pred$predictions
  j <- j+1

  ########################################################

  rf.fit <- ranger(y = dat.tr[, 1], x =  Ktrain2, importance = "impurity")

  rf.pred <- predict(rf.fit , data = Ktest2)

  RMSE[i,j] <- calcRMSE(dat.te[,1], rf.pred$predictions)
  predictions[,j] <- rf.pred$predictions
  j <- j+1

  ########################################################

  svm.fit <- svm(V1~.,dat.tr[, -1062])

  #Predict using SVM regression
  svm.pred <- predict(svm.fit, dat.te[, -1062])
  RMSE[i,j] <- calcRMSE(dat.te[,1], svm.pred)
  predictions[,j] <- svm.pred
  j <- j+1

  ########################################################

  rf.fit <- ranger(y = dat.tr[, 1], x =  cbind(kpcKern2$KPCs[ , 1 : 6], dat.tr$class), importance = "impurity")

  rf.pred <- predict(rf.fit , data = cbind(KPCpred3[ , 1 : 6], dat.te$class))

  RMSE[i,j] <- calcRMSE(dat.te[,1], rf.pred$predictions)
  predictions[,j] <- rf.pred$predictions
  j <- j+1

  ########################################################
# 
  RMSE[i,j] <-  calcRMSE(dat.te[,1],   rowMeans(predictions[, 1:13]))
}

plot(dat.te[,1],rowMeans(predictions[, 1:13]),  col = as.numeric(dat.te$class)+1, xlab = "kappa_casein validation set: one random split", ylab = "predicted",  main = "ensemble")
abline(0,1)
```


## kappa_casein



```{r, fig.cap="kappa_casein results", echo=FALSE, message=FALSE, warning=FALSE}
colnames(RMSE) <- c("lasso", "lm+pca", "pls3", "pls4","pls5",  "lm+15", "rf", "rf+vs", "bart", "lm+kpca","rf+kpca", "rf+kernel", "svm", "rf+kpca+class", "ensemble")

RMSE1 <- RMSE  %>% 
  as.data.frame() %>% 
  mutate(split = as.factor(1:N))%>% pivot_longer(1:15,"ALGORITHM")

# RMSE1 %>% ggplot(aes(x = ALGORITHM, y = value)) + geom_boxplot()
RMSE1 %>% 
  group_by(ALGORITHM) %>% 
  mutate(meanv = mean(value)) %>% 
  ggplot(aes(x= ALGORITHM, y = value, color = split, group = split)) +
  geom_point() + 
  geom_line() +
  geom_line(aes(x = ALGORITHM, y = meanv), col = 1) +
  theme(legend.position = "none") +
  ylab("RMSE")

t1 <- RMSE1 %>% 
  group_by(ALGORITHM) %>% 
  summarise(meanv = mean(value), sdv=sd(value)) %>%
  arrange(desc(meanv))



```


## Casein_micelle_size



```{r fig.cap="Casein_micelle_size", echo=FALSE, message=FALSE, warning=FALSE}


dat <- cbind(log(y[,2]), tr)
dat <- as.data.frame(dat)
dat <- dat %>% mutate(class = as.factor(labl.Ave))
dat <- dat %>% na.omit()
nonz <- nonz2

RMSE <- matrix(0, N, 15)
predictions <- matrix(0,nrow(dat)-200, 14)

for (i in 1:N){
  j <- 1
  dat.tr <- dat[random_split[,i],]
  dat.te <- dat[-random_split[,i],]

  ########################################################

  x <- model.matrix(V1 ~. , data = dat.tr[, -1062])
  y1 <- dat.tr[,1]

  grid <- 10^seq(-3, 3, length = 100)


  test.exp <- exp(dat.te[,1])

  lasso.fit <- glmnet(x,y1,alpha=1, lambda = grid) # for
  cv.out <- cv.glmnet(x,y1,alpha=1)


  lasso.fit <- glmnet(x,y1,alpha=1,
                      lambda = cv.out$lambda.min)

  lasso.pred <- (predict(lasso.fit, newx = model.matrix(V1 ~. , data = dat.te[, -1062])))
  # plot((test.exp), (lasso.pred))
  # abline(a = 0, b = 1)
  RMSE[i,j] <- calcRMSE(test.exp, exp(lasso.pred))
  predictions[,j] <- lasso.pred
  j <- j + 1

  ########################################################

  pc <- prcomp(dat.tr[, 2:1061], scale = TRUE)

  lm.fit<- lm(dat.tr[, 1]~pc$x[,1:5])

  x.te <- predict(pc, dat.te[, 2:1061])

  lm.pred <- as.matrix(cbind(rep(1, nrow(x.te)), x.te[,1:5])) %*% as.matrix(lm.fit$coef[1:6])

  RMSE[i,j] <- calcRMSE(test.exp, exp(lm.pred))
  predictions[,j] <- lm.pred
  j <- j + 1

  ########################################################

  pls.fit <- plsr(V1 ~., data=dat.tr[, -1062], scale= TRUE, validation = "CV")

  pls.pred <- predict(pls.fit, dat.te[,-1], ncomp = 3)

  RMSE[i,j] <- calcRMSE(test.exp, exp(pls.pred))
  predictions[,j] <- pls.pred
  j <- j+ 1

  
  pls.pred <- predict(pls.fit, dat.te[,-1], ncomp = 4)

  RMSE[i,j] <- calcRMSE(test.exp, exp(pls.pred))
  predictions[,j] <- pls.pred
  j <- j+ 1

  pls.pred <- predict(pls.fit, dat.te[,-1], ncomp = 5)

  RMSE[i,j] <- calcRMSE(test.exp, exp(pls.pred))
  predictions[,j] <- pls.pred
  j <- j+ 1

  ########################################################

  lm.fit <- lm(dat.tr[, 1]~ as.matrix(dat.tr[, nonz+1]) )

  x <- model.matrix(V1 ~. , data = dat.te[, c(1, nonz+1)])

  lm.pred <- x %*% as.matrix(lm.fit$coef)

  RMSE[i,j] <- calcRMSE(test.exp, exp(lm.pred))
  predictions[,j] <- lm.pred
  j <- j+ 1

  ########################################################

  rf.fit <- ranger(V1 ~ ., data = dat.tr, importance = "impurity")

  rf.pred <- predict(rf.fit , data = dat.te)

  RMSE[i,j] <- calcRMSE(test.exp, exp(rf.pred$predictions))
  predictions[,j] <- rf.pred$predictions
  j <- j+1

  ########################################################

  rf.fit <- ranger(V1 ~ ., data = dat.tr, importance = "impurity", regularization.factor = 0.2, regularization.usedepth=FALSE)
  # plot(rf.fit$variable.importance)
  rf.pred <- predict(rf.fit , data = dat.te)

  RMSE[i,j] <- calcRMSE(test.exp, exp(rf.pred$predictions))
  predictions[,j] <- rf.pred$predictions
  j <- j+1


  ########################################################

  bart.fit <- bartMachine(as.data.frame(dat.tr[,-1]), dat.tr[,1], verbose = FALSE)
  # summary(bart.fit)
  bart.pred <- predict(bart.fit, as.data.frame(dat.te[,-1]))
  RMSE[i,j] <- calcRMSE(test.exp, exp(bart.pred))
  predictions[,j] <- bart.pred
  j <-j+1

  ########################################################


  Ktrain2 <- kernelMatrix(kfunc, as.matrix(dat.tr[,2:1061]))
  Ktest2 <- kernelMatrix(kfunc, as.matrix(dat.te[,2:1061]), as.matrix(dat.tr[,2:1061]))
 kpcKern2 <- kPCA(Ktrain2)


  # plot the data projection on the principal components

  # pairs(cbind(dat.tr[,1], kpcKern2$KPCs[ , 1 : 6]), col = dat.tr$class)
  KPCpred3 <- predict(kpcKern2, Ktest2)
  # pairs(KPCpred3[ , 1 : 6], col = dat.te$class)
  lm.fit <- lm(dat.tr[, 1]~  kpcKern2$KPCs[ , 1 : 6] )

  lm.pred <- cbind(rep(1,nrow(KPCpred3)), KPCpred3[ , 1 : 6]) %*% as.matrix(lm.fit$coef)
  # plot(test.exp, lm.pred , col = as.numeric(dat.te$class), main = "KPCs")
  # abline(0,1)

  RMSE[i,j] <- calcRMSE(test.exp, exp(lm.pred))
  predictions[,j] <- lm.pred
  j <- j+ 1


  ########################################################


  rf.fit <- ranger(y = dat.tr[, 1], x =  kpcKern2$KPCs[ , 1 : 6], importance = "impurity")
  # plot(rf.fit$variable.importance)
  rf.pred <- predict(rf.fit , data = KPCpred3[ , 1 : 6])

  RMSE[i,j] <- calcRMSE(test.exp, exp(rf.pred$predictions))
  predictions[,j] <- rf.pred$predictions
  j <- j+1

  ########################################################


  rf.fit <- ranger(y = dat.tr[, 1], x =  Ktrain2, importance = "impurity")
  # plot(rf.fit$variable.importance)
  rf.pred <- predict(rf.fit , data = Ktest2)

  RMSE[i,j] <- calcRMSE(test.exp, exp(rf.pred$predictions))
  predictions[,j] <- rf.pred$predictions
  j <- j+1
  # plot(test.exp, rf.pred$predictions, col = as.numeric(dat.te$class), main = "RF")
  # abline(0,1)

  ########################################################

  svm.fit <- svm(V1~.,dat.tr[, -1062])

  #Predict using SVM regression
  svm.pred <- predict(svm.fit, dat.te[, -1062])
  RMSE[i,j] <- calcRMSE(test.exp, exp(svm.pred))
  predictions[,j] <- svm.pred
  j <- j+1
  # plot(test.exp, svm.pred, col = as.numeric(dat.te$class), main = "RF")
  # abline(0,1)
  ########################################################

  rf.fit <- ranger(y = dat.tr[, 1], x =  cbind(kpcKern2$KPCs[ , 1 : 6], dat.tr$class), importance = "impurity")
  # plot(rf.fit$variable.importance)
  rf.pred <- predict(rf.fit , data = cbind(KPCpred3[ , 1 : 6], dat.te$class))

  RMSE[i,j] <- calcRMSE(test.exp, exp(rf.pred$predictions))
  predictions[,j] <- rf.pred$predictions
  j <- j+1

  #######################################################

  # plot(test.exp,   rowMeans(predictions[, 1:13]))
  # abline(a=0, b=1)  
  RMSE[i, j] <- calcRMSE(test.exp,   exp(rowMeans(predictions[, 1 : 13])))
}


colnames(RMSE) <- c("lasso", "lm+pca", "pls3", "pls4","pls5",  "lm+15", "rf", "rf+vs", "bart", "lm+kpca","rf+kpca", "rf+kernel", "svm", "rf+kpca+class", "ensemble")



RMSE2 <- RMSE  %>% 
  as.data.frame() %>% 
  mutate(split = as.factor(1:N)) %>% pivot_longer(1:15,"ALGORITHM")
# RMSE1 %>% ggplot(aes(x = ALGORITHM, y = value)) + geom_boxplot()

RMSE2 %>% 
  filter(ALGORITHM!="ensemble") %>%
  filter(ALGORITHM!="lasso") %>% 
  group_by(ALGORITHM) %>% 
  mutate(meanv = mean(value)) %>% 
  ggplot(aes(x= ALGORITHM, y = value, color = split, group = split)) +
  geom_point() + 
  geom_line() +
  geom_line(aes(x = ALGORITHM, y = meanv), col = 1) +
  theme(legend.position = "none") +
  ylab("RMSE")

t2 <- RMSE2 %>% 
  group_by(ALGORITHM) %>% 
  summarise(meanv = mean(value, na.rm=TRUE)*10, sdv=sd(value)) %>% 
  arrange(desc(meanv))



```


## Native_pH



```{r fig.cap="Native_pH", echo=FALSE, message=FALSE, warning=FALSE}


dat <- cbind((y[,3]), tr)
dat <- as.data.frame(dat)
dat <- dat %>% mutate(class = as.factor(labl.Ave))
dat <- dat %>% na.omit()
nonz <- nonz3

RMSE <- matrix(0, N, 15)
predictions <- matrix(0,nrow(dat)-200, 14)


for (i in 1:N){
  j <- 1
  dat.tr <- dat[random_split[,i],]
  dat.te <- dat[-random_split[,i],]

  ########################################################

  x <- model.matrix(V1 ~. , data = dat.tr[, -1062])
  y1 <- dat.tr[,1]

  grid <- 10^seq(-3, 3, length = 100)


  test.exp <- exp(dat.te[,1])

  lasso.fit <- glmnet(x,y1,alpha=1, lambda = grid) # for
  cv.out <- cv.glmnet(x,y1,alpha=1)


  lasso.fit <- glmnet(x,y1,alpha=1,
                      lambda = cv.out$lambda.min)

  lasso.pred <- (predict(lasso.fit, newx = model.matrix(V1 ~. , data = dat.te[, -1062])))
  # plot((test.exp), (lasso.pred))
  # abline(a = 0, b = 1)
  RMSE[i,j] <- calcRMSE(test.exp, exp(lasso.pred))
  predictions[,j] <- lasso.pred
  j <- j + 1

  ########################################################

  pc <- prcomp(dat.tr[, 2:1061], scale = TRUE)

  lm.fit<- lm(dat.tr[, 1]~pc$x[,1:5])

  x.te <- predict(pc, dat.te[, 2:1061])

  lm.pred <- as.matrix(cbind(rep(1, nrow(x.te)), x.te[,1:5])) %*% as.matrix(lm.fit$coef[1:6])

  RMSE[i,j] <- calcRMSE(test.exp, exp(lm.pred))
  predictions[,j] <- lm.pred
  j <- j + 1

  ########################################################

  pls.fit <- plsr(V1 ~., data=dat.tr[, -1062], scale= TRUE, validation = "CV")

  pls.pred <- predict(pls.fit, dat.te[,-1], ncomp = 3)

  RMSE[i,j] <- calcRMSE(test.exp, exp(pls.pred))
  predictions[,j] <- pls.pred
  j <- j+ 1

  
  pls.pred <- predict(pls.fit, dat.te[,-1], ncomp = 4)

  RMSE[i,j] <- calcRMSE(test.exp, exp(pls.pred))
  predictions[,j] <- pls.pred
  j <- j+ 1

  pls.pred <- predict(pls.fit, dat.te[,-1], ncomp = 5)

  RMSE[i,j] <- calcRMSE(test.exp, exp(pls.pred))
  predictions[,j] <- pls.pred
  j <- j+ 1

  ########################################################

  lm.fit <- lm(dat.tr[, 1]~ as.matrix(dat.tr[, nonz+1]) )

  x <- model.matrix(V1 ~. , data = dat.te[, c(1, nonz+1)])

  lm.pred <- x %*% as.matrix(lm.fit$coef)

  RMSE[i,j] <- calcRMSE(test.exp, exp(lm.pred))
  predictions[,j] <- lm.pred
  j <- j+ 1

  ########################################################

  rf.fit <- ranger(V1 ~ ., data = dat.tr, importance = "impurity")

  rf.pred <- predict(rf.fit , data = dat.te)

  RMSE[i,j] <- calcRMSE(test.exp, exp(rf.pred$predictions))
  predictions[,j] <- rf.pred$predictions
  j <- j+1

  ########################################################

  rf.fit <- ranger(V1 ~ ., data = dat.tr, importance = "impurity", regularization.factor = 0.2, regularization.usedepth=FALSE)
  # plot(rf.fit$variable.importance)
  rf.pred <- predict(rf.fit , data = dat.te)

  RMSE[i,j] <- calcRMSE(test.exp, exp(rf.pred$predictions))
  predictions[,j] <- rf.pred$predictions
  j <- j+1


  ########################################################

  bart.fit <- bartMachine(as.data.frame(dat.tr[,-1]), dat.tr[,1], verbose = FALSE)
  # summary(bart.fit)
  bart.pred <- predict(bart.fit, as.data.frame(dat.te[,-1]))
  RMSE[i,j] <- calcRMSE(test.exp, exp(bart.pred))
  predictions[,j] <- bart.pred
  j <-j+1

  ########################################################


  Ktrain2 <- kernelMatrix(kfunc, as.matrix(dat.tr[,2:1061]))
  Ktest2 <- kernelMatrix(kfunc, as.matrix(dat.te[,2:1061]), as.matrix(dat.tr[,2:1061]))
 kpcKern2 <- kPCA(Ktrain2)


  # plot the data projection on the principal components

  # pairs(cbind(dat.tr[,1], kpcKern2$KPCs[ , 1 : 6]), col = dat.tr$class)
  KPCpred3 <- predict(kpcKern2, Ktest2)
  # pairs(KPCpred3[ , 1 : 6], col = dat.te$class)
  lm.fit <- lm(dat.tr[, 1]~  kpcKern2$KPCs[ , 1 : 6] )

  lm.pred <- cbind(rep(1,nrow(KPCpred3)), KPCpred3[ , 1 : 6]) %*% as.matrix(lm.fit$coef)
  # plot(test.exp, lm.pred , col = as.numeric(dat.te$class), main = "KPCs")
  # abline(0,1)

  RMSE[i,j] <- calcRMSE(test.exp, exp(lm.pred))
  predictions[,j] <- lm.pred
  j <- j+ 1


  ########################################################


  rf.fit <- ranger(y = dat.tr[, 1], x =  kpcKern2$KPCs[ , 1 : 6], importance = "impurity")
  # plot(rf.fit$variable.importance)
  rf.pred <- predict(rf.fit , data = KPCpred3[ , 1 : 6])

  RMSE[i,j] <- calcRMSE(test.exp, exp(rf.pred$predictions))
  predictions[,j] <- rf.pred$predictions
  j <- j+1

  ########################################################


  rf.fit <- ranger(y = dat.tr[, 1], x =  Ktrain2, importance = "impurity")
  # plot(rf.fit$variable.importance)
  rf.pred <- predict(rf.fit , data = Ktest2)

  RMSE[i,j] <- calcRMSE(test.exp, exp(rf.pred$predictions))
  predictions[,j] <- rf.pred$predictions
  j <- j+1
  # plot(test.exp, rf.pred$predictions, col = as.numeric(dat.te$class), main = "RF")
  # abline(0,1)

  ########################################################

  svm.fit <- svm(V1~.,dat.tr[, -1062])

  #Predict using SVM regression
  svm.pred <- predict(svm.fit, dat.te[, -1062])
  RMSE[i,j] <- calcRMSE(test.exp, exp(svm.pred))
  predictions[,j] <- svm.pred
  j <- j+1
  # plot(test.exp, svm.pred, col = as.numeric(dat.te$class), main = "RF")
  # abline(0,1)
  ########################################################

  rf.fit <- ranger(y = dat.tr[, 1], x =  cbind(kpcKern2$KPCs[ , 1 : 6], dat.tr$class), importance = "impurity")
  # plot(rf.fit$variable.importance)
  rf.pred <- predict(rf.fit , data = cbind(KPCpred3[ , 1 : 6], dat.te$class))

  RMSE[i,j] <- calcRMSE(test.exp, exp(rf.pred$predictions))
  predictions[,j] <- rf.pred$predictions
  j <- j+1

  #######################################################

  # plot(test.exp,   rowMeans(predictions[, 1:13]))
  # abline(a=0, b=1)  
  RMSE[i, j] <- calcRMSE(test.exp,   exp(rowMeans(predictions[, 1 : 13])))
}

colnames(RMSE) <- c("lasso", "lm+pca", "pls3", "pls4","pls5",  "lm+15", "rf", "rf+vs", "bart", "lm+kpca","rf+kpca", "rf+kernel", "svm", "rf+kpca+class", "ensemble")


RMSE3 <- RMSE  %>%
  as.data.frame() %>%
  mutate(split = as.factor(1:N)) %>%
  pivot_longer(1:15,"ALGORITHM")


RMSE3 %>% 
  group_by(ALGORITHM) %>% 
  mutate(meanv = mean(value)) %>% 
  ggplot(aes(x= ALGORITHM, y = value, color = split, group = split)) +
  geom_point()+ geom_line() +
  geom_line(aes(x = ALGORITHM, y = meanv), col = 1) +
  theme(legend.position = "none") +
  ylab("RMSE")

t3 <- RMSE3 %>% 
  group_by(ALGORITHM) %>% 
  summarise(meanv = mean(value), sdv=sd(value)) %>% 
  arrange(desc(meanv))

```



## Thank you

- code at https://github.com/domijan/vistamilk


