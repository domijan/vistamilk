---
title: "Vista Milk: predictions from an ensemble model"
author: Katarina Domijan
date: April 29, 2021
output: beamer_presentation
editor_options: 
  chunk_output_type: console
---


## Data:

```{r echo = FALSE, message=FALSE, warning=FALSE, results = 'hide'}
set.seed(1979)
library(tidyverse)
library(dendextend)
library(glmnet) # for the ridge regression
library(pls)
library(ranger)
library(bartMachine)
library(e1071)
library(BKPC)
library(kernlab)
tr <- read.csv("tr.csv") 
te <- read.csv("te.csv") 

calcRMSE <- function(y, yhat){sqrt(mean((yhat - y)^2))}


y <- tr[,1:3]
tr <- tr[, -c(1:3)]


a <- apply(tr,2,mean)
b <- apply(tr,2,sd)


tr <- scale(tr,a,b)
te <- scale(te,a,b)


kfunc <-  rbfdot(sigma = 0.0005)

```



- The data were scaled and centered

```{r fig.cap="Scaled training data", echo=FALSE}
###########################################
# Look for clusters in the training data


D <- as.matrix(dist(tr,method="euclidean"))


h <- hclust(as.dist(D), method = "ward.D2")
d <- as.dendrogram(h)


labl.Ave <- cutree(d,7)
matplot(t(tr), type = "l", col = labl.Ave, lty = 1, ylab = "training data") # see the 7 clusters


```



## Cross - validation set up:

- Split the full training set into 100 random splits of sample size 200 (training) and 353 (validation).
- Fit the models to the randomly selected training (sub)-sets and compare using RMSE of the validation sets. 



## Classifiers

Submitted predictions are from an ensemble model -  averages taken over predictions from the following models:


1. pls with 3, 4, and 5 components (R package **pls**)
2. linear regression with 6 PCs with highest eigenvalues
3. lasso (R package **glmnet**)
4. random forest (R package **ranger**) with and without regularization
5. BART (R package **bartMachine**)

## Classifiers cont'd

6. support vector machine for regression (R package **e1071**)
7. linear regression with 6 KPCs with highest eigenvalues using Gaussian kernel (R package **kernlab** and **BKPC**)
8. random forest with 6 KPCs with highest eigenvalues using Gaussian kernel
9. random forest with a Gaussian kernel 
10. linear regression with 15 wavelengths selected by a) clustering the wavelengths into 15 clusters and selecting the wavelength from each cluster that has the highest correlation with the response variable


```{r echo=FALSE}
# clustering the columns (variables) rather than observations
D2 <- as.matrix(1-abs(cor(tr)))

# image(D)


h2<- hclust(as.dist(D2), method = "ward.D2")
dnew <- as.dendrogram(h2)


indexcor <- abs(cor(cbind(y,tr), use = "pairwise.complete.obs"))[1:3, -c(1:3)]



labl.cols <- cutree(dnew,15) # 15 variables


# Pick 15 top features
nonz1 <- NULL
nonz2 <- NULL
nonz3 <- NULL
for (j in unique(labl.cols)){
  nonz1[j] <- (which(indexcor[1,]==max(indexcor[1,labl.cols==j])))
  nonz2[j] <- (which(indexcor[2,]==max(indexcor[2,labl.cols==j])))
  nonz3[j] <- (which(indexcor[3,]==max(indexcor[3,labl.cols==j])))
  
  
  }
```

## kappa_casein

```{r echo = FALSE}
N <- 50
```


```{r, fig.cap="kappa_casein", echo=FALSE, message=FALSE, warning=FALSE}


dat <- cbind((y[,1]), tr)
dat <- as.data.frame(dat)
dat <- dat %>% mutate(class = as.factor(labl.Ave))
dat <- dat %>% na.omit()
nonz <- nonz1

ttt <- replicate(N, sample(nrow(dat), 200))




RMSE <- matrix(0, N, 15)
predictions <- matrix(0,nrow(dat)-200, 14)
for (i in 1:N){
  j <- 1
  dat.tr <- dat[ttt[,i],]
  dat.te <- dat[-ttt[,i],]

  
  x <- model.matrix(V1 ~. , data = dat.tr[, -1062])
  
  y1 <- dat.tr[,1]
  
  grid <- 10^seq(-3, 3, length = 100)
  
  
  
  
  lasso.fit <- glmnet(x,y1,alpha=1, lambda = grid) # for 
  cv.out <- cv.glmnet(x,y1,alpha=1)

  
  lasso.fit <- glmnet(x,y1,alpha=1, 
                      lambda = cv.out$lambda.min)
 
  
  
  lasso.pred <- predict(lasso.fit, newx = model.matrix(V1 ~. , data = dat.te[, -1062]))
  RMSE[i,j] <- calcRMSE(dat.te[,1], lasso.pred)
  predictions[,j] <- lasso.pred 
  j <- j + 1

 

  
  
  
  
  pc <- prcomp(dat.tr[, 2:1061], scale = TRUE)

  
  
  lmfit<- lm(dat.tr[, 1]~pc$x[,1:5])

  x.te <- predict(pc, dat.te[, 2:1061])

  pca.pred.poor.te <- as.matrix(cbind(rep(1, nrow(x.te)), x.te[,1:5])) %*% as.matrix(lmfit$coef[1:6])

  RMSE[i,j] <- calcRMSE(dat.te[,1], pca.pred.poor.te)
  predictions[,j] <- pca.pred.poor.te 
  j <- j + 1


  
  pls.fit <- plsr(V1 ~., data=dat.tr[, -1062], scale= TRUE, validation = "CV")

  pls.pred <- predict(pls.fit, dat.te[,-1], ncomp = 3)
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pls.pred)
  predictions[,j] <- pls.pred 
  j <- j+ 1
  
  pls.pred <- predict(pls.fit, dat.te[,-1], ncomp = 4)
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pls.pred)
  predictions[,j] <- pls.pred 
  j <- j+ 1
  
  pls.pred <- predict(pls.fit, dat.te[,-1], ncomp = 5)
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pls.pred)
  predictions[,j] <- pls.pred 
  j <- j+ 1

  lmfit <- lm(dat.tr[, 1]~ as.matrix(dat.tr[, nonz+1]) )

  
  x <- model.matrix(V1 ~. , data = dat.te[, c(1, nonz+1)])
  
  
  
  pca.pred.poor.te <- x %*% as.matrix(lmfit$coef)

  
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pca.pred.poor.te)
  predictions[,j] <- pca.pred.poor.te 
  j <- j+ 1

  
  
  
  rf.fit <- ranger(V1 ~ ., data = dat.tr, importance = "impurity")

  pred.rf <- predict(rf.fit , data = dat.te)
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pred.rf$predictions)
  predictions[,j] <- pred.rf$predictions 
  j <- j+1

  
  
  rf.fit <- ranger(V1 ~ ., data = dat.tr, importance = "impurity",regularization.factor = 0.2, regularization.usedepth=FALSE)
  # plot(rf.fit$variable.importance)
  pred.rf <- predict(rf.fit , data = dat.te)
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pred.rf$predictions)
  predictions[,j] <- pred.rf$predictions
  j <- j+1

  
  
  
  bart_machine = bartMachine(as.data.frame(dat.tr[,-1]), dat.tr[,1], verbose = FALSE)
  # summary(bart_machine)
  pred.bart <- predict(bart_machine, as.data.frame(dat.te[,-1]))
  RMSE[i,j] <- calcRMSE(dat.te[,1], pred.bart)
  predictions[,j] <- pred.bart
  j <-j+1

  
  
  Ktrain2 <- kernelMatrix(kfunc, as.matrix(dat.tr[,2:1061]))
  Ktest2 <- kernelMatrix(kfunc, as.matrix(dat.te[,2:1061]), as.matrix(dat.tr[,2:1061]))
 kpcKern2 <- kPCA(Ktrain2)
  

  KPCpred3 <- predict(kpcKern2, Ktest2)
  # pairs(KPCpred3[ , 1 : 6], col = dat.te$class)
  lmfit <- lm(dat.tr[, 1]~  kpcKern2$KPCs[ , 1 : 6] )

 
  
  
  
  pca.pred.poor.te <- cbind(rep(1,nrow(KPCpred3)), KPCpred3[ , 1 : 6]) %*% as.matrix(lmfit$coef)

  
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pca.pred.poor.te)
  predictions[,j] <- pca.pred.poor.te
  j <- j+ 1
  
 
 
  
  rf.fit <- ranger(y = dat.tr[, 1], x =  kpcKern2$KPCs[ , 1 : 6], importance = "impurity")

  pred.rf <- predict(rf.fit , data = KPCpred3[ , 1 : 6])
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pred.rf$predictions)
  predictions[,j] <- pred.rf$predictions
  j <- j+1
  
  
  
  rf.fit <- ranger(y = dat.tr[, 1], x =  Ktrain2, importance = "impurity")

  pred.rf <- predict(rf.fit , data = Ktest2)
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pred.rf$predictions)
  predictions[,j] <- pred.rf$predictions
  j <- j+1



  modelsvm <- svm(V1~.,dat.tr[, -1062])
  
  #Predict using SVM regression
  predYsvm = predict(modelsvm, dat.te[, -1062])
  RMSE[i,j] <- calcRMSE(dat.te[,1], predYsvm)
  predictions[,j] <- predYsvm
  j <- j+1

  rf.fit <- ranger(y = dat.tr[, 1], x =  cbind(kpcKern2$KPCs[ , 1 : 6], dat.tr$class), importance = "impurity")

  pred.rf <- predict(rf.fit , data = cbind(KPCpred3[ , 1 : 6], dat.te$class))
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pred.rf$predictions)
  predictions[,j] <- pred.rf$predictions
  j <- j+1
  
  
  RMSE[i,j] <-  calcRMSE(dat.te[,1],   rowMeans(predictions[, 1:13]))
}


colnames(RMSE) <- c("lasso", "lm+pca", "pls3", "pls4","pls5",  "lm+15", "rf", "rf+vs", "bart", "lm+kpca","rf+kpca", "rf+kernel", "svm", "rf+kpca+class", "ensamble")

RMSE1 <- RMSE  %>%as.data.frame()%>% mutate(split = as.factor(1:N))%>% pivot_longer(1:15,"ALGORITHM")

# RMSE1 %>% ggplot(aes(x = ALGORITHM, y = value)) + geom_boxplot()
RMSE1 %>% group_by(ALGORITHM) %>% mutate(meanv = mean(value)) %>% ggplot(aes(x= ALGORITHM, y = value, color = split, group = split))+
  geom_point()+ geom_line() +geom_line(aes(x = ALGORITHM, y = meanv), col = 1)+ theme(legend.position = "none")+
  ylab("RMSE")

t1 <- RMSE1 %>% group_by(ALGORITHM) %>% summarise(meanv = mean(value), sdv=sd(value)) %>% arrange(desc(meanv))
 


# RMSE2 <- RMSE  %>%as.data.frame()%>% mutate(split = as.factor(1:N))%>% pivot_longer(1:15,"ALGORITHM")
# # RMSE1 %>% ggplot(aes(x = ALGORITHM, y = value)) + geom_boxplot()
# RMSE2 %>% group_by(ALGORITHM) %>% mutate(meanv = mean(value)) %>% ggplot(aes(x= ALGORITHM, y = value, color = split, group = split))+
#   geom_point()+ geom_line() +geom_line(aes(x = ALGORITHM, y = meanv), col = 1)+ theme(legend.position = "none")+
#   ylab("RMSE")
# 
# t2 <- RMSE2 %>% group_by(ALGORITHM) %>% summarise(meanv = mean(value), sdv=sd(value)) %>% arrange(desc(meanv))
# 
# 
# 
# RMSE3 <- RMSE  %>%as.data.frame()%>% mutate(split = as.factor(1:N))%>% pivot_longer(1:15,"ALGORITHM")
# RMSE1 %>% ggplot(aes(x = ALGORITHM, y = value)) + geom_boxplot()
# RMSE3 %>% group_by(ALGORITHM) %>% mutate(meanv = mean(value)) %>% ggplot(aes(x= ALGORITHM, y = value, color = split, group = split))+
#   geom_point()+ geom_line() +geom_line(aes(x = ALGORITHM, y = meanv), col = 1)+ theme(legend.position = "none")+
#   ylab("RMSE")
# 
# t3 <- RMSE3 %>% group_by(ALGORITHM) %>% summarise(meanv = mean(value), sdv=sd(value)) %>% arrange(desc(meanv))


```


## Casein_micelle_size



```{r fig.cap="Casein_micelle_size", echo=FALSE, message=FALSE, warning=FALSE}


dat <- cbind(log(y[,2]), tr)
dat <- as.data.frame(dat)
dat <- dat %>% mutate(class = as.factor(labl.Ave))
dat <- dat %>% na.omit()
nonz <- nonz2





RMSE <- matrix(0, N, 15)
predictions <- matrix(0,nrow(dat)-200, 14)
for (i in 1:N){
  j <- 1
  dat.tr <- dat[ttt[,i],]
  dat.te <- dat[-ttt[,i],]

  
  x <- model.matrix(V1 ~. , data = dat.tr[, -1062])
  
  y1 <- dat.tr[,1]
  
  grid <- 10^seq(-3, 3, length = 100)
  
  
  
  
  lasso.fit <- glmnet(x,y1,alpha=1, lambda = grid) # for 
  cv.out <- cv.glmnet(x,y1,alpha=1)

  
  lasso.fit <- glmnet(x,y1,alpha=1, 
                      lambda = cv.out$lambda.min)
 
  
  
  lasso.pred <- predict(lasso.fit, newx = model.matrix(V1 ~. , data = dat.te[, -1062]))
  RMSE[i,j] <- calcRMSE(dat.te[,1], lasso.pred)
  predictions[,j] <- lasso.pred 
  j <- j + 1

 

  
  
  
  
  pc <- prcomp(dat.tr[, 2:1061], scale = TRUE)

  
  
  lmfit<- lm(dat.tr[, 1]~pc$x[,1:5])

  x.te <- predict(pc, dat.te[, 2:1061])

  pca.pred.poor.te <- as.matrix(cbind(rep(1, nrow(x.te)), x.te[,1:5])) %*% as.matrix(lmfit$coef[1:6])

  RMSE[i,j] <- calcRMSE(dat.te[,1], pca.pred.poor.te)
  predictions[,j] <- pca.pred.poor.te 
  j <- j + 1


  
  pls.fit <- plsr(V1 ~., data=dat.tr[, -1062], scale= TRUE, validation = "CV")

  pls.pred <- predict(pls.fit, dat.te[,-1], ncomp = 3)
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pls.pred)
  predictions[,j] <- pls.pred 
  j <- j+ 1
  
  pls.pred <- predict(pls.fit, dat.te[,-1], ncomp = 4)
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pls.pred)
  predictions[,j] <- pls.pred 
  j <- j+ 1
  
  pls.pred <- predict(pls.fit, dat.te[,-1], ncomp = 5)
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pls.pred)
  predictions[,j] <- pls.pred 
  j <- j+ 1

  lmfit <- lm(dat.tr[, 1]~ as.matrix(dat.tr[, nonz+1]) )

  
  x <- model.matrix(V1 ~. , data = dat.te[, c(1, nonz+1)])
  
  
  
  pca.pred.poor.te <- x %*% as.matrix(lmfit$coef)

  
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pca.pred.poor.te)
  predictions[,j] <- pca.pred.poor.te 
  j <- j+ 1

  
  
  
  rf.fit <- ranger(V1 ~ ., data = dat.tr, importance = "impurity")

  pred.rf <- predict(rf.fit , data = dat.te)
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pred.rf$predictions)
  predictions[,j] <- pred.rf$predictions 
  j <- j+1

  
  
  rf.fit <- ranger(V1 ~ ., data = dat.tr, importance = "impurity",regularization.factor = 0.2, regularization.usedepth=FALSE)
  # plot(rf.fit$variable.importance)
  pred.rf <- predict(rf.fit , data = dat.te)
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pred.rf$predictions)
  predictions[,j] <- pred.rf$predictions
  j <- j+1

  
  
  
  bart_machine = bartMachine(as.data.frame(dat.tr[,-1]), dat.tr[,1], verbose = FALSE)
  # summary(bart_machine)
  pred.bart <- predict(bart_machine, as.data.frame(dat.te[,-1]))
  RMSE[i,j] <- calcRMSE(dat.te[,1], pred.bart)
  predictions[,j] <- pred.bart
  j <-j+1

  
  
  Ktrain2 <- kernelMatrix(kfunc, as.matrix(dat.tr[,2:1061]))
  Ktest2 <- kernelMatrix(kfunc, as.matrix(dat.te[,2:1061]), as.matrix(dat.tr[,2:1061]))
 kpcKern2 <- kPCA(Ktrain2)
  
  
  # plot the data projection on the principal components
  
  # pairs(cbind(dat.tr[,1], kpcKern2$KPCs[ , 1 : 6]), col = dat.tr$class)
  KPCpred3 <- predict(kpcKern2, Ktest2)
  # pairs(KPCpred3[ , 1 : 6], col = dat.te$class)
  lmfit <- lm(dat.tr[, 1]~  kpcKern2$KPCs[ , 1 : 6] )

 
  
  
  
  pca.pred.poor.te <- cbind(rep(1,nrow(KPCpred3)), KPCpred3[ , 1 : 6]) %*% as.matrix(lmfit$coef)
  # plot(dat.te[,1], pca.pred.poor.te , col = as.numeric(dat.te$class), main = "KPCs")
  # abline(0,1)
  
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pca.pred.poor.te)
  predictions[,j] <- pca.pred.poor.te
  j <- j+ 1
  
 
 
  
  rf.fit <- ranger(y = dat.tr[, 1], x =  kpcKern2$KPCs[ , 1 : 6], importance = "impurity")
  # plot(rf.fit$variable.importance)
  pred.rf <- predict(rf.fit , data = KPCpred3[ , 1 : 6])
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pred.rf$predictions)
  predictions[,j] <- pred.rf$predictions
  j <- j+1
  
  
  
  rf.fit <- ranger(y = dat.tr[, 1], x =  Ktrain2, importance = "impurity")
  # plot(rf.fit$variable.importance)
  pred.rf <- predict(rf.fit , data = Ktest2)
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pred.rf$predictions)
  predictions[,j] <- pred.rf$predictions
  j <- j+1
  # plot(dat.te[,1], pred.rf$predictions, col = as.numeric(dat.te$class), main = "RF")
  # abline(0,1)


  modelsvm <- svm(V1~.,dat.tr[, -1062])
  
  #Predict using SVM regression
  predYsvm = predict(modelsvm, dat.te[, -1062])
  RMSE[i,j] <- calcRMSE(dat.te[,1], predYsvm)
  predictions[,j] <- predYsvm
  j <- j+1
  # plot(dat.te[,1], predYsvm, col = as.numeric(dat.te$class), main = "RF")
  # abline(0,1)
  rf.fit <- ranger(y = dat.tr[, 1], x =  cbind(kpcKern2$KPCs[ , 1 : 6], dat.tr$class), importance = "impurity")
  # plot(rf.fit$variable.importance)
  pred.rf <- predict(rf.fit , data = cbind(KPCpred3[ , 1 : 6], dat.te$class))
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pred.rf$predictions)
  predictions[,j] <- pred.rf$predictions
  j <- j+1
  
  
  RMSE[i,j] <-  calcRMSE(dat.te[,1],   rowMeans(predictions[, 1:13]))
}


colnames(RMSE) <- c("lasso", "lm+pca", "pls3", "pls4","pls5",  "lm+15", "rf", "rf+vs", "bart", "lm+kpca","rf+kpca", "rf+kernel", "svm", "rf+kpca+class", "ensamble")

# RMSE1 <- RMSE  %>%as.data.frame()%>% mutate(split = as.factor(1:N))%>% pivot_longer(1:15,"ALGORITHM")
# 
# # RMSE1 %>% ggplot(aes(x = ALGORITHM, y = value)) + geom_boxplot()
# RMSE1 %>% group_by(ALGORITHM) %>% mutate(meanv = mean(value)) %>% ggplot(aes(x= ALGORITHM, y = value, color = split, group = split))+
#   geom_point()+ geom_line() +geom_line(aes(x = ALGORITHM, y = meanv), col = 1)+ theme(legend.position = "none")+
#   ylab("RMSE")
# 
# t1 <- RMSE1 %>% group_by(ALGORITHM) %>% summarise(meanv = mean(value), sdv=sd(value)) %>% arrange(desc(meanv))
 


RMSE2 <- RMSE  %>%as.data.frame()%>% mutate(split = as.factor(1:N))%>% pivot_longer(1:15,"ALGORITHM")
# RMSE1 %>% ggplot(aes(x = ALGORITHM, y = value)) + geom_boxplot()
RMSE2 %>% group_by(ALGORITHM) %>% mutate(meanv = mean(value)) %>% ggplot(aes(x= ALGORITHM, y = value, color = split, group = split))+
  geom_point()+ geom_line() +geom_line(aes(x = ALGORITHM, y = meanv), col = 1)+ theme(legend.position = "none")+
  ylab("RMSE")

t2 <- RMSE2 %>% group_by(ALGORITHM) %>% summarise(meanv = mean(value), sdv=sd(value)) %>% arrange(desc(meanv))
# 
# 
# 
# RMSE3 <- RMSE  %>%as.data.frame()%>% mutate(split = as.factor(1:N))%>% pivot_longer(1:15,"ALGORITHM")
# RMSE1 %>% ggplot(aes(x = ALGORITHM, y = value)) + geom_boxplot()
# RMSE3 %>% group_by(ALGORITHM) %>% mutate(meanv = mean(value)) %>% ggplot(aes(x= ALGORITHM, y = value, color = split, group = split))+
#   geom_point()+ geom_line() +geom_line(aes(x = ALGORITHM, y = meanv), col = 1)+ theme(legend.position = "none")+
#   ylab("RMSE")
# 
# t3 <- RMSE3 %>% group_by(ALGORITHM) %>% summarise(meanv = mean(value), sdv=sd(value)) %>% arrange(desc(meanv))


```


## Native_pH



```{r fig.cap="Native_pH", echo=FALSE, message=FALSE, warning=FALSE}


dat <- cbind((y[,3]), tr)
dat <- as.data.frame(dat)
dat <- dat %>% mutate(class = as.factor(labl.Ave))
dat <- dat %>% na.omit()
nonz <- nonz3





RMSE <- matrix(0, N, 15)
predictions <- matrix(0,nrow(dat)-200, 14)
for (i in 1:N){
  j <- 1
  dat.tr <- dat[ttt[,i],]
  dat.te <- dat[-ttt[,i],]

  
  x <- model.matrix(V1 ~. , data = dat.tr[, -1062])
  
  y1 <- dat.tr[,1]
  
  grid <- 10^seq(-3, 3, length = 100)
  
  
  
  
  lasso.fit <- glmnet(x,y1,alpha=1, lambda = grid) # for 
  cv.out <- cv.glmnet(x,y1,alpha=1)

  
  lasso.fit <- glmnet(x,y1,alpha=1, 
                      lambda = cv.out$lambda.min)
 
  
  
  lasso.pred <- predict(lasso.fit, newx = model.matrix(V1 ~. , data = dat.te[, -1062]))
  RMSE[i,j] <- calcRMSE(dat.te[,1], lasso.pred)
  predictions[,j] <- lasso.pred 
  j <- j + 1

 

  
  
  
  
  pc <- prcomp(dat.tr[, 2:1061], scale = TRUE)

  
  
  lmfit<- lm(dat.tr[, 1]~pc$x[,1:5])

  x.te <- predict(pc, dat.te[, 2:1061])

  pca.pred.poor.te <- as.matrix(cbind(rep(1, nrow(x.te)), x.te[,1:5])) %*% as.matrix(lmfit$coef[1:6])

  RMSE[i,j] <- calcRMSE(dat.te[,1], pca.pred.poor.te)
  predictions[,j] <- pca.pred.poor.te 
  j <- j + 1


  
  pls.fit <- plsr(V1 ~., data=dat.tr[, -1062], scale= TRUE, validation = "CV")

  pls.pred <- predict(pls.fit, dat.te[,-1], ncomp = 3)
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pls.pred)
  predictions[,j] <- pls.pred 
  j <- j+ 1
  
  pls.pred <- predict(pls.fit, dat.te[,-1], ncomp = 4)
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pls.pred)
  predictions[,j] <- pls.pred 
  j <- j+ 1
  
  pls.pred <- predict(pls.fit, dat.te[,-1], ncomp = 5)
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pls.pred)
  predictions[,j] <- pls.pred 
  j <- j+ 1

  lmfit <- lm(dat.tr[, 1]~ as.matrix(dat.tr[, nonz+1]) )

  
  x <- model.matrix(V1 ~. , data = dat.te[, c(1, nonz+1)])
  
  
  
  pca.pred.poor.te <- x %*% as.matrix(lmfit$coef)

  
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pca.pred.poor.te)
  predictions[,j] <- pca.pred.poor.te 
  j <- j+ 1

  
  
  
  rf.fit <- ranger(V1 ~ ., data = dat.tr, importance = "impurity")

  pred.rf <- predict(rf.fit , data = dat.te)
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pred.rf$predictions)
  predictions[,j] <- pred.rf$predictions 
  j <- j+1

  
  
  rf.fit <- ranger(V1 ~ ., data = dat.tr, importance = "impurity",regularization.factor = 0.2, regularization.usedepth=FALSE)
  # plot(rf.fit$variable.importance)
  pred.rf <- predict(rf.fit , data = dat.te)
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pred.rf$predictions)
  predictions[,j] <- pred.rf$predictions
  j <- j+1

  
  
  
  bart_machine = bartMachine(as.data.frame(dat.tr[,-1]), dat.tr[,1], verbose = FALSE)
  # summary(bart_machine)
  pred.bart <- predict(bart_machine, as.data.frame(dat.te[,-1]))
  RMSE[i,j] <- calcRMSE(dat.te[,1], pred.bart)
  predictions[,j] <- pred.bart
  j <-j+1

  
  
  Ktrain2 <- kernelMatrix(kfunc, as.matrix(dat.tr[,2:1061]))
  Ktest2 <- kernelMatrix(kfunc, as.matrix(dat.te[,2:1061]), as.matrix(dat.tr[,2:1061]))
 kpcKern2 <- kPCA(Ktrain2)
  
  
  # plot the data projection on the principal components
  
  # pairs(cbind(dat.tr[,1], kpcKern2$KPCs[ , 1 : 6]), col = dat.tr$class)
  KPCpred3 <- predict(kpcKern2, Ktest2)
  # pairs(KPCpred3[ , 1 : 6], col = dat.te$class)
  lmfit <- lm(dat.tr[, 1]~  kpcKern2$KPCs[ , 1 : 6] )
  # plot(lmfit,1)
  # anova(lmfit)
  # summary(lmfit)
  # predict(lmfit, KPCpred3[ , 1 : 6])
 
  
  
  
  pca.pred.poor.te <- cbind(rep(1,nrow(KPCpred3)), KPCpred3[ , 1 : 6]) %*% as.matrix(lmfit$coef)
  # plot(dat.te[,1], pca.pred.poor.te , col = as.numeric(dat.te$class), main = "KPCs")
  # abline(0,1)
  
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pca.pred.poor.te)
  predictions[,j] <- pca.pred.poor.te
  j <- j+ 1
  
 
 
  
  rf.fit <- ranger(y = dat.tr[, 1], x =  kpcKern2$KPCs[ , 1 : 6], importance = "impurity")
  # plot(rf.fit$variable.importance)
  pred.rf <- predict(rf.fit , data = KPCpred3[ , 1 : 6])
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pred.rf$predictions)
  predictions[,j] <- pred.rf$predictions
  j <- j+1
  
  
  
  rf.fit <- ranger(y = dat.tr[, 1], x =  Ktrain2, importance = "impurity")
  # plot(rf.fit$variable.importance)
  pred.rf <- predict(rf.fit , data = Ktest2)
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pred.rf$predictions)
  predictions[,j] <- pred.rf$predictions
  j <- j+1
  # plot(dat.te[,1], pred.rf$predictions, col = as.numeric(dat.te$class), main = "RF")
  # abline(0,1)


  modelsvm <- svm(V1~.,dat.tr[, -1062])
  
  #Predict using SVM regression
  predYsvm = predict(modelsvm, dat.te[, -1062])
  RMSE[i,j] <- calcRMSE(dat.te[,1], predYsvm)
  predictions[,j] <- predYsvm
  j <- j+1
  # plot(dat.te[,1], predYsvm, col = as.numeric(dat.te$class), main = "RF")
  # abline(0,1)
  rf.fit <- ranger(y = dat.tr[, 1], x =  cbind(kpcKern2$KPCs[ , 1 : 6], dat.tr$class), importance = "impurity")
  # plot(rf.fit$variable.importance)
  pred.rf <- predict(rf.fit , data = cbind(KPCpred3[ , 1 : 6], dat.te$class))
  
  RMSE[i,j] <- calcRMSE(dat.te[,1], pred.rf$predictions)
  predictions[,j] <- pred.rf$predictions
  j <- j+1
  
  
  RMSE[i,j] <-  calcRMSE(dat.te[,1],   rowMeans(predictions[, 1:13]))
}

colnames(RMSE) <- c("lasso", "lm+pca", "pls3", "pls4","pls5",  "lm+15", "rf", "rf+vs", "bart", "lm+kpca","rf+kpca", "rf+kernel", "svm", "rf+kpca+class", "ensamble")


# RMSE1 <- RMSE  %>%as.data.frame()%>% mutate(split = as.factor(1:N))%>% pivot_longer(1:15,"ALGORITHM")
# 
# # RMSE1 %>% ggplot(aes(x = ALGORITHM, y = value)) + geom_boxplot()
# RMSE1 %>% group_by(ALGORITHM) %>% mutate(meanv = mean(value)) %>% ggplot(aes(x= ALGORITHM, y = value, color = split, group = split))+
#   geom_point()+ geom_line() +geom_line(aes(x = ALGORITHM, y = meanv), col = 1)+ theme(legend.position = "none")+
#   ylab("RMSE")
# 
# t1 <- RMSE1 %>% group_by(ALGORITHM) %>% summarise(meanv = mean(value), sdv=sd(value)) %>% arrange(desc(meanv))
#  


# RMSE2 <- RMSE  %>%as.data.frame()%>% mutate(split = as.factor(1:N))%>% pivot_longer(1:15,"ALGORITHM")
# # RMSE1 %>% ggplot(aes(x = ALGORITHM, y = value)) + geom_boxplot()
# RMSE2 %>% group_by(ALGORITHM) %>% mutate(meanv = mean(value)) %>% ggplot(aes(x= ALGORITHM, y = value, color = split, group = split))+
#   geom_point()+ geom_line() +geom_line(aes(x = ALGORITHM, y = meanv), col = 1)+ theme(legend.position = "none")+
#   ylab("RMSE")
# 
# t2 <- RMSE2 %>% group_by(ALGORITHM) %>% summarise(meanv = mean(value), sdv=sd(value)) %>% arrange(desc(meanv))
# 
# 
# 
RMSE3 <- RMSE  %>%as.data.frame()%>% mutate(split = as.factor(1:N))%>% pivot_longer(1:15,"ALGORITHM")
# RMSE1 %>% ggplot(aes(x = ALGORITHM, y = value)) + geom_boxplot()
RMSE3 %>% group_by(ALGORITHM) %>% mutate(meanv = mean(value)) %>% ggplot(aes(x= ALGORITHM, y = value, color = split, group = split))+
  geom_point()+ geom_line() +geom_line(aes(x = ALGORITHM, y = meanv), col = 1)+ theme(legend.position = "none")+
  ylab("RMSE")

t3 <- RMSE3 %>% group_by(ALGORITHM) %>% summarise(meanv = mean(value), sdv=sd(value)) %>% arrange(desc(meanv))


```

<!-- ## kappa_casein -->
<!-- ```{r} -->

<!-- ``` -->

## Thank you

- code at https://github.com/domijan/vistamilk


