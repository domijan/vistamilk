---
title: "Analysis with Ensambles: 14 traits data (absorbance values)"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Use all 14 traits

Compared to the competition: 

* Only a subset of columns is used
* The transmittance values are transformed to absorbance values.

## Compared to the older version: 

* ALl traits are scaled and centered
* PLS with optimal no of components
* Added lasso to stack regressions (enslasso)

```{r echo = FALSE, message=FALSE, warning= FALSE}
rm(list = ls())
set.seed(1979)
library(tidyverse)
library(dendextend)
library(glmnet) # for the ridge regression
library(pls)
library(ranger)
library(bartMachine)
library(e1071)
library(BKPC)
library(kernlab)
library(caret) # For ten fold crossvalidation
library(effects)
library (stats)
library(lme4)
library(brnn)
library(gbm)

```

```{r echo = FALSE, message=FALSE, warning= FALSE, eval = TRUE}

a <- read.csv("recleaneddataset1.csv") 


y_names <- c("kappa_casein",
"alpha_s2_casein",
"alpha_s1_casein",
"beta_casein",
"alpha_lactalbumin",
"beta_lactoglobulin_a",
"beta_lactoglobulin_b",
"Casein_micelle_size",
"Heat_stability",
"Native_pH",
"RCT",
"k20",
"a30",
"a60")

ys <- a %>% select(y_names)
x <- a[,60:590] 
# matplot(t(x), type = "l", col = as.numeric(as.factor(a$Breed))+1)
matplot(t(x), type = "l", col = a$Farm+1)
# matplot(t(log(1/tr[,4:1060],10)), type = "l")
calcRMSE <- function(y, yhat){sqrt(mean((yhat - y)^2))}

ys[,8] <- log(ys[,8])
# pairs(ys)
a <- apply(ys,2,mean, na.rm = TRUE)
b <- apply(ys,2,sd, na.rm = TRUE)

ys <- sweep(ys,2,a,"-")
ys <- sweep(ys,2,b,"/")
#names(a)
id_te <- sample(nrow(x), 69)
ys_te <- ys[id_te,]
te <- x[id_te, ]
ys_tr <- ys[-id_te,]
tr <- x[-id_te, ]

me <- apply(tr,2,mean)
sd <- apply(tr,2,sd)




# pairs(log(ys)) # all non-normal
ys %>% pivot_longer(1:14, "key", "value") %>% ggplot(aes(x = value))+ 
  geom_histogram()+facet_wrap(vars(key), scales = "free")

# matplot(t(tr), type = "l", col = 9, lty = 1)
# matplot(t(te), type = "l", col = 9, lty = 1)


# pairs(tr[,1:10])


tr <- scale(tr,me,sd)
te <- scale(te,me,sd)
# matplot(t(tr), type = "l", col = a$Farm[-id_te]+1, lty = 1)
# matplot(t(te), type = "l", col = as.numeric(as.factor(a$Breed[-id_te]))+1, lty = 1)


```


<!-- ## Look for clusters in the training data: rows -->

```{r echo = FALSE, message=FALSE, warning= FALSE, eval = TRUE}
D <- as.matrix(dist(tr,method="euclidean"))
# image(D)


h <- hclust(as.dist(D), method = "ward.D2")
d <- as.dendrogram(h)
# plot(d)
# 
# heatmap(D,
#         Colv=as.dendrogram(h),     
#         Rowv=as.dendrogram(h))


d2 <- color_branches(d,k=7, col=c(2,3,5,4, 6,7,8)) 
# auto-coloring 4 clusters of branches.
# plot(d2)

# plot(reorder(d2, D, method = "OLO")) 

# labl.Ave <- cutree(d2,7)
# matplot(t(tr), type = "l", col = labl.Ave, lty = 1, ylab = "training data") # see the 7 clusters

# table(a$Farm[-id_te], labl.Ave)
# table(a$Breed[-id_te], labl.Ave)

#############################################################

```


<!-- ## Look for clusters in the training data: columns -->

```{r echo = FALSE, message=FALSE, warning= FALSE, eval = TRUE}
#############################################################

kfunc <-  rbfdot(sigma = 0.0005)
# kfunc <- laplacedot(sigma = 0.0001)
# kfunc <- anovadot(sigma = 1, degree = 1)
Ktrain <- kernelMatrix(kfunc, tr)
# image(Ktrain)

Ktest <- kernelMatrix(kfunc, te, tr)



kpcTrain <- kPCA(Ktrain)


# plot the data projection on the principal components

# pairs(kpcTrain$KPCs[ , 1 : 6], col = labl.Ave)

# newdat <- cbind(log(ys_tr), kpcTrain$KPCs[ , 1 : 6], labl.Ave)
# 
# newdat <- newdat %>% as.data.frame() %>% na.omit() %>% mutate(labl.Ave = as.factor(labl.Ave))
# pairs(newdat[,1:20],  col = newdat[,21])
kpcTest <- predict(kpcTrain, Ktest)
# pairs(kpcTest[ , 1 : 6])
#############################################################

# correlation with responses

indexcor <- abs(cor(cbind(ys_tr,tr), use = "pairwise.complete.obs"))[1:14, -c(1:14)]
# matplot(t(indexcor), type = "l", col = 9, lty = 1)




# clustering the columns (variables) rather than observations
D2 <- as.matrix(1-abs(cor(tr)))

# image(D2)


h2<- hclust(as.dist(D2), method = "ward.D2")
dnew <- as.dendrogram(h2)

# heatmap(D2,
#         Colv=as.dendrogram(h2),     
#         Rowv=as.dendrogram(h2))


d2 <- color_branches(dnew,k=15, col = 2:16) 
# auto-coloring 4 clusters of branches.
 plot(d2)

# plot(reorder(d2, D2, method = "OLO")) 

labl.cols <- cutree(d2,15) # 15 variables
# matplot(t(tr), type = "l", col = labl.Ave, lty = 1)
# points(1:1060, rep(-5.8, 1060), col = labl.cols)

# image(t(indexcor))
# Pick 15 top features
nonz <- matrix(0,14,15)


for (j in unique(labl.cols)){
  for (i in 1:14){nonz[i, j] <- (which(indexcor[i,]==max(indexcor[i,labl.cols==j])))}
  }
  
  

# matplot(t(tr), type = "l", col = labl.Ave, lty = 1)
# abline(v = nonz[1,], col = 1:15) # pick values in a sensible region

# max(indexcor[1,])

# pairs(cbind(ys_tr[,1], tr[,nonz[1,]]), col = labl.Ave)
```


## Add more ensamble classifiers



* MA: model averaging
* Stack: regression
* Stack: non negative coefficients
* Stack: lasso
* Stack: RF and all the predictors


### Crossvalidation set up

* Split the data into training and testing (200 observations for the training), the rest for testing, 50 random splits. 

* 10 fold cross - validation on each of the 50 training sets to train stack. 


```{r echo = FALSE, message=FALSE, warning= FALSE, eval = TRUE}

N <- 50
grid <- 10^seq(-3, 3, length = 100)
RMSEFULL <- vector(mode = "list", length = 20)
t1 <- vector(mode = "list", length = 14)
names(t1)<- names(ys_tr)
flds <- createFolds(1:200, k = 10, list = TRUE, returnTrain = FALSE)


pls.pc <-c(4, 6, 5, 4, 3, 6, 15, 4, 10, 15,8, 4,8,7)
  
lm.fit.SG <- replicate(14, vector(mode = "list", length = N))
nonneg.fit.SG<- replicate(14, vector(mode = "list", length = N))
rf.fit.SG <- replicate(14, vector(mode = "list", length = N)) 
lasso.fit.SG <- replicate(14, vector(mode = "list", length = N)) 
  
  
for(yind in 1:14){
  dat <- cbind(ys_tr[,yind], tr)
  
  dat <- as.data.frame(dat)
  p <- dat %>% ggplot(aes(x= V1)) + geom_histogram() + ggtitle(names(ys_tr)[yind])
  print(p)
  dat <- dat %>% na.omit()
  
  nonzy <- nonz[yind,]
  
  set.seed(1979)
  ttt <- replicate(N, sample(nrow(dat), 200))
 
  
  RMSE <- matrix(0, N, 20)
  predictions.te <- matrix(0, nrow(dat)-200, 15)
  predictions.tv <- matrix(0, 200, 15)
 


  for (i in 1:N){


    dat.tr.full <- dat[ttt[,i],]
    dat.te <- dat[-ttt[,i],]
 
    for(l in 1:10){
        dat.tr <- dat.tr.full[-flds[[l]],]
        dat.tv <- dat.tr.full[flds[[l]],]
        
        j <- 1
        x <- model.matrix(V1 ~. , data = dat.tr)
        y1 <- dat.tr[,1]
        
        set.seed(1951)
        lasso.fit <- glmnet(x,y1,alpha=1, lambda = grid) # for lasso
        cv.out <- cv.glmnet(x,y1,alpha=1)
        # print(cv.out$lambda.min)
        
        set.seed(1951)
        lasso.fit <- glmnet(x,y1,alpha=1, 
                            lambda = cv.out$lambda.min)
        
        
        lasso.pred <- predict(lasso.fit, newx = model.matrix(V1 ~. , data = dat.tv))
        predictions.tv[flds[[l]] ,j] <- lasso.pred 
        j <- j + 1
        
        #############################################
        
        set.seed(1951)
        en.fit <- glmnet(x,y1,alpha=0.5, lambda = grid) # for lasso
        
        # plot(lasso.fit)
        
        cv.out <- cv.glmnet(x,y1,alpha=0.5)
        # print(cv.out$lambda.min)
        
        set.seed(1951)
        en.fit <- glmnet(x,y1,alpha=0.5, 
                         lambda = cv.out$lambda.min)
        
        
        en.pred <- predict(en.fit, newx = model.matrix(V1 ~. , data = dat.tv))
        predictions.tv[flds[[l]] ,j] <- en.pred 
        j <- j + 1
        
        
        # cbind(coef(lasso.fit), coef(en.fit))
        ####################################################
        set.seed(1951)
        pc <- prcomp(dat.tr[, 2:ncol(dat.tr)], scale = TRUE)
        
        pca.lm.fit<- lm(dat.tr[, 1]~pc$x[,1:5])
        
        x.tv <- predict(pc, dat.tv[, 2:ncol(dat.tr)])
        # pca.pred.poor <- as.matrix(cbind(rep(1, 200), pc$x[,1:5])) %*% as.matrix(pca.lm.fit$coef[1:6])
        pca.pred <- as.matrix(cbind(rep(1, nrow(x.tv)), x.tv[,1:5])) %*% as.matrix(pca.lm.fit$coef[1:6])
        
        predictions.tv[flds[[l]] ,j] <- pca.pred 
        j <- j + 1
        # plot(dat.tv[,1], pca.pred, col = as.numeric(dat.tv$class), main = "PCA")
        # abline(a = 0, b = 1)
        
        #############################################################
        
        set.seed(1951)
        pls.fit <- plsr(V1 ~., data=dat.tr, scale= TRUE, validation = "CV")
        
        
        pls.pred <- predict(pls.fit, dat.tv[,-1], ncomp = pls.pc[yind])
        
        predictions.tv[flds[[l]] ,j] <- pls.pred 
        j <- j+ 1
        

        #############################################################
        set.seed(1951)
        lm.fit <- lm(dat.tr[, 1]~ as.matrix(dat.tr[, nonzy+1]) )
        x <- model.matrix(V1 ~. , data = dat.tv[, c(1, nonzy+1)])
        lm.pred <- x %*% as.matrix(lm.fit$coef)
        
        
        predictions.tv[flds[[l]] ,j] <- lm.pred
        j <- j+ 1
        
        #############################################################
        set.seed(1951) 
        
        rf.fit <- ranger(V1 ~ ., data = dat.tr, importance = "impurity")
        # plot(rf.fit$variable.importance)
        pred.rf <- predict(rf.fit , data = dat.tv)
        
        predictions.tv[flds[[l]] ,j] <- pred.rf$predictions 
        j <- j+1
        
        ####################################################
        set.seed(1951)      
        rf.vs.fit <- ranger(V1 ~ ., data = dat.tr, importance = "impurity",regularization.factor = 0.2, regularization.usedepth=FALSE)
        # plot(rf.fit$variable.importance)
        pred.vs.rf <- predict(rf.vs.fit , data = dat.tv)
        
        predictions.tv[flds[[l]] ,j] <- pred.vs.rf$predictions
        j <- j+1
        # plot(dat.tv[,1], pred.rf$predictions, col = as.numeric(dat.tv$class))
        # abline(0,1)
        
        ####################################################
        set.seed(1951)
        bart.fit <- bartMachine(as.data.frame(dat.tr[,-1]), dat.tr[,1], verbose = FALSE)
        # summary(bart.fit)
        pred.bart <- predict(bart.fit, as.data.frame(dat.tv[,-1]))
        predictions.tv[flds[[l]] ,j] <- pred.bart
        j <-j+1
        # plot(dat.tv[,1], pred.bart, col = as.numeric(dat.tv$class), main = "BART")
        # abline(0,1)
        
        ####################################################
        set.seed(1951)        
        Ktrain <- kernelMatrix(kfunc, as.matrix(dat.tr[,2:ncol(dat.tr)]))
        Kvalid <- kernelMatrix(kfunc, as.matrix(dat.tv[,2:ncol(dat.tr)]), as.matrix(dat.tr[,2:ncol(dat.tr)]))
        kpcTrain <- kPCA(Ktrain)
        
        
        # plot the data projection on the principal components
        
        # pairs(cbind(dat.tr[,1], kpcTrain$KPCs[ , 1 : 6]), col = dat.tr$class)
        kpcValid <- predict(kpcTrain, Kvalid)
        # pairs(kpcTest[ , 1 : 6], col = dat.tv$class)
        kpca.lm.fit <- lm(dat.tr[, 1]~  kpcTrain$KPCs[ , 1 : 6] )
        
        kpca.pred <- cbind(rep(1,nrow(kpcValid)), kpcValid[ , 1 : 6]) %*% as.matrix(kpca.lm.fit$coef)
        # plot(dat.tv[,1], kpca.pred , col = as.numeric(dat.tv$class), main = "KPCs")
        # abline(0,1)
        
        predictions.tv[flds[[l]] ,j] <- kpca.pred
        j <- j+ 1
        
        
        ####################################################
        set.seed(1951)
        
        rf.kpc.fit <- ranger(y = dat.tr[, 1], x =  kpcTrain$KPCs[ , 1 : 6], importance = "impurity")
        # plot(rf.fit$variable.importance)
        pred.rf.kpc <- predict(rf.kpc.fit , data = kpcValid[ , 1 : 6])
        predictions.tv[flds[[l]] ,j] <- pred.rf.kpc$predictions
        j <- j+1
        
        
        ####################################################
        set.seed(1951)        
        rf.k.fit <- ranger(y = dat.tr[, 1], x =  Ktrain, importance = "impurity")
        # plot(rf.fit$variable.importance)
        pred.rf.k <- predict(rf.k.fit , data = Kvalid)
        predictions.tv[flds[[l]] ,j] <- pred.rf.k$predictions
        j <- j+1
        
        
        ####################################################
        set.seed(1951)        
        svm.fit <- svm(V1~.,dat.tr)
        
        #Predict using SVM regression
        pred.svm <- predict(svm.fit, dat.tv)
        predictions.tv[flds[[l]] ,j] <- pred.svm
        j <- j+1
        # plot(dat.tv[,1], predYsvm,  main = "RF")
        # abline(0,1)
        
     
        ####################################################
        set.seed(1951)
        ppr.fit <- ppr(dat.tr[,-1], dat.tr[, 1], nterms=1, max.terms=2)
        pred.ppr <- ppr.fit %>% predict(dat.tv[,-1])               

        predictions.tv[flds[[l]] ,j] <- pred.ppr
        j <- j+1
        # plot(dat.tv[,1], pred.ppr,  main = "RF")
        # abline(0,1)
        ####################################################
        set.seed(1951)        
        brnn.fit <- brnn(as.matrix(dat.tr[, -1]), dat.tr[, 1])
        ###prediction
        pred.brnn <- brnn.fit %>% predict(dat.tv[,-1])                          #predict y hat
        predictions.tv[flds[[l]] ,j] <- pred.brnn
        j <- j+1
        # plot(dat.tv[,1], pred.brnn,  main = "BRNN")
        # abline(0,1)
        
        ####################################################
        set.seed(1951)       
        gbm.fit <- gbm(V1~., data = dat.tr, distribution = "gaussian", cv.folds = 5, n.trees = 200)
        ###prediction
        best.iter <- gbm.perf(gbm.fit, method = "cv")

        pred.gbm <- gbm.fit %>% predict(dat.tv[,-1],  n.trees = best.iter)                          #predict y hat
        predictions.tv[flds[[l]] ,j] <- pred.gbm
        j <- j+1
        # plot(dat.tv[,1], pred.gbm,  main = "GBM")
        # abline(0,1)
        
    }
    
     
    colnames(predictions.tv) <- c("lasso", "elastic net", "pca", "pls",  "LM+15", "RF", "RF+VS", "Bart", "kPCA","RF+kPCA", "RF+kernel", "svm", "ppr", "brnn", "gbm")
    # pairs(cbind(dat.tr.full[,1], predictions.tv)) # correlated predictions
    
    
    ####################################################
    set.seed(1951)    
    lm.fit.SG[[i]][[yind]]<- lm(dat.tr.full[,1]~predictions.tv - 1) 
     ####################################################
    set.seed(1951)    
    nonneg.fit.SG[[i]][[yind]] <- glmnet(predictions.tv,dat.tr.full[,1],alpha=1, lambda = 0, lower.limits = 0, intercept = FALSE) 
    ####################################################
    set.seed(1951) 
    rf.fit.SG[[i]][[yind]] <- ranger(y = dat.tr.full[, 1], x = cbind( predictions.tv, dat.tr.full[, -1]), importance = "impurity")
     ####################################################
    set.seed(1951)    
    cv.out <- cv.glmnet(predictions.tv,dat.tr.full[,1],alpha=1, lambda = grid,  intercept = FALSE)
    # plot(cv.out)
    lasso.fit.SG[[i]][[yind]] <- glmnet(predictions.tv,dat.tr.full[,1],alpha=1, lambda = cv.out$lambda.min,  intercept = FALSE)
   
    # plot(coef(nonneg.fit.SG[[i]][[yind]]), coef(lasso.fit.SG[[i]][[yind]]))
      # sum(coef(lasso.fit.SG[[i]][[yind]]))
    
      # pred.stack.lm <- as.matrix( predictions.tv) %*% as.matrix(lm.fit.SG[[i]][[yind]]$coef)
 
 # pred.stack.nonneg <- predict(nonneg.fit.SG[[i]][[yind]], predictions.te)
    

  # pred.stack.rf <- predict(lm.fit.SG[[i]][[yind]], data = cbind(predictions.te, dat.te[, -1]))$prediction
  # 
  #  pred.stack.lasso <- predict(lasso.fit.SG[[i]][[yind]], predictions.te)
    
    # plot(dat.tr.full[, 1], as.matrix( predictions.tv) %*% as.matrix(lm.fit.SG[[i]][[yind]]$coef))
    #   abline(a = 0, b = 1)  
      
      #   plot(dat.tr.full[, 1], predict(nonneg.fit.SG[[i]][[yind]], predictions.tv))
      # abline(a = 0, b = 1)  
      
      #    plot(dat.tr.full[, 1], predict(rf.fit.SG[[i]][[yind]], data = cbind(predictions.tv, dat.tr.full[, -1]))$prediction)
      # abline(a = 0, b = 1)  
      
    # plot(coef(lasso.fit.SG[[i]][[yind]]), coef(nonneg.fit.SG[[i]][[yind]]))
      # plot(dat.tr.full[, 1], predict(lasso.fit.SG[[i]][[yind]], predictions.tv))
      # abline(a = 0, b = 1)
    ##-----------------------------------------------------------
    j <- 1
    ####################################################
    set.seed(1951) 
    x <- model.matrix(V1 ~. , data = dat.tr.full)
    y1 <- dat.tr.full[,1]
    lasso.fit <- glmnet(x,y1,alpha=1, lambda = grid) # for lasso
    cv.out <- cv.glmnet(x,y1,alpha=1)
    # print(cv.out$lambda.min)
    
    lasso.fit <- glmnet(x,y1,alpha=1, 
                        lambda = cv.out$lambda.min)
    
    
    lasso.pred <- predict(lasso.fit, newx = model.matrix(V1 ~. , data = dat.te))
    RMSE[i,j] <- calcRMSE(dat.te[,1], lasso.pred)
    predictions.te[,j] <- lasso.pred 
    j <- j + 1
    
    #############################################
    ####################################################
    set.seed(1951)   
    en.fit <- glmnet(x,y1,alpha=0.5, lambda = grid) # for lasso
    
    # plot(lasso.fit)
    
    cv.out <- cv.glmnet(x,y1,alpha=0.5)
    # print(cv.out$lambda.min)
    
    en.fit <- glmnet(x,y1,alpha=0.5, 
                     lambda = cv.out$lambda.min)
    
    
    en.pred <- predict(en.fit, newx = model.matrix(V1 ~. , data = dat.te))
    RMSE[i,j] <- calcRMSE(dat.te[,1], en.pred)
    predictions.te[,j] <- en.pred 
    j <- j + 1
    
    ####################################################
    ####################################################
    set.seed(1951)    
    pc <- prcomp(dat.tr.full[, 2:ncol(dat.tr.full)], scale = TRUE)
    
    pca.lm.fit<- lm(dat.tr.full[, 1]~pc$x[,1:5])
    
    x.te <- predict(pc, dat.te[, 2:ncol(dat.tr.full)])
    # pca.pred.poor <- as.matrix(cbind(rep(1, 200), pc$x[,1:5])) %*% as.matrix(pca.lm.fit$coef[1:6])
    pca.pred <- as.matrix(cbind(rep(1, nrow(x.te)), x.te[,1:5])) %*% as.matrix(pca.lm.fit$coef[1:6])
    
    RMSE[i,j] <- calcRMSE(dat.te[,1], pca.pred)
    predictions.te[,j] <- pca.pred
    j <- j + 1
    # plot(dat.te[,1], pca.pred, col = as.numeric(dat.te$class), main = "PCA")
    # abline(a = 0, b = 1)
    
    #############################################################
      
    set.seed(1951)   
    
    pls.fit <- plsr(V1 ~., data=dat.tr.full, scale= TRUE, validation = "CV")
    
    
    pls.pred <- predict(pls.fit, dat.te[,-1], ncomp = pls.pc[yind])
    
    RMSE[i,j] <- calcRMSE(dat.te[,1], pls.pred)
    predictions.te[,j] <- pls.pred 
    j <- j+ 1
    
    
    #############################################################
    set.seed(1951)     
    lm.fit <- lm(dat.tr.full[, 1]~ as.matrix(dat.tr.full[, nonzy+1]) )
    x <- model.matrix(V1 ~. , data = dat.te[, c(1, nonzy+1)])
    lm.pred <- x %*% as.matrix(lm.fit$coef)
    
    
    RMSE[i,j] <- calcRMSE(dat.te[,1], lm.pred)
    predictions.te[,j] <- lm.pred 
    j <- j+ 1
    
    #############################################################
    
    
    set.seed(1951)   
    rf.fit <- ranger(V1 ~ ., data = dat.tr.full, importance = "impurity")
    # plot(rf.fit$variable.importance)
    pred.rf <- predict(rf.fit , data = dat.te)
    
    RMSE[i,j] <- calcRMSE(dat.te[,1], pred.rf$predictions)
    predictions.te[,j] <- pred.rf$predictions 
    j <- j+1
    #############################################################
    
    
    set.seed(1951)   
    
    
    rf.vs.fit <- ranger(V1 ~ ., data = dat.tr.full, importance = "impurity",regularization.factor = 0.2, regularization.usedepth=FALSE)
    # plot(rf.fit$variable.importance)
    pred.rf.vs <- predict(rf.vs.fit , data = dat.te)
    
    RMSE[i,j] <- calcRMSE(dat.te[,1], pred.rf.vs$predictions)
    predictions.te[,j] <- pred.rf.vs$predictions
    j <- j+1
    # plot(dat.te[,1], pred.rf$predictions, col = as.numeric(dat.te$class))
    # abline(0,1)
    
    #############################################################
    
    
    set.seed(1951)   
    
    
    bart.fit = bartMachine(as.data.frame(dat.tr.full[,-1]), dat.tr.full[,1], verbose = FALSE)
    # summary(bart.fit)
    pred.bart <- predict(bart.fit, as.data.frame(dat.te[,-1]))
    RMSE[i,j] <- calcRMSE(dat.te[,1], pred.bart)
    predictions.te[,j] <- pred.bart
    j <-j+1
    # plot(dat.te[,1], pred.bart, col = as.numeric(dat.te$class), main = "BART")
    # abline(0,1)
    
    #############################################################
    
    
    set.seed(1951)   
    
    Ktrain <- kernelMatrix(kfunc, as.matrix(dat.tr.full[,2:ncol(dat.tr.full)]))
    Ktest <- kernelMatrix(kfunc, as.matrix(dat.te[,2:ncol(dat.tr.full)]), as.matrix(dat.tr.full[,2:ncol(dat.tr.full)]))
    kpcTrain <- kPCA(Ktrain)
    
    
    # plot the data projection on the principal components
    
    # pairs(cbind(dat.tr.full[,1], kpcTrain$KPCs[ , 1 : 6]), col = dat.tr.full$class)
    kpcTest <- predict(kpcTrain, Ktest)
    # pairs(kpcTest[ , 1 : 6], col = dat.te$class)
    kpca.lm.fit <- lm(dat.tr.full[, 1]~  kpcTrain$KPCs[ , 1 : 6] )
    
    kpca.pred <- cbind(rep(1,nrow(kpcTest)), kpcTest[ , 1 : 6]) %*% as.matrix(kpca.lm.fit$coef)
    # plot(dat.te[,1], kpca.pred , col = as.numeric(dat.te$class), main = "KPCs")
    # abline(0,1)
    
    
    RMSE[i,j] <- calcRMSE(dat.te[,1], kpca.pred)
    predictions.te[,j] <- kpca.pred
    j <- j+ 1
    
    
    
    #############################################################
    set.seed(1951)   
   
    rf.kpca.fit <- ranger(y = dat.tr.full[, 1], x =  kpcTrain$KPCs[ , 1 : 6], importance = "impurity")
    # plot(rf.fit$variable.importance)
    pred.rf.kpca <- predict(rf.kpca.fit , data = kpcTest[ , 1 : 6])
    
    RMSE[i,j] <- calcRMSE(dat.te[,1], pred.rf.kpca$predictions)
    predictions.te[,j] <- pred.rf.kpca$predictions
    j <- j+1
    
    #############################################################
    set.seed(1951)    
    
    rf.k.fit <- ranger(y = dat.tr.full[, 1], x =  Ktrain, importance = "impurity")
    # plot(rf.fit$variable.importance)
    pred.rf.k <- predict(rf.k.fit , data = Ktest)
    
    RMSE[i,j] <- calcRMSE(dat.te[,1], pred.rf.k$predictions)
    predictions.te[,j] <- pred.rf.k$predictions
    j <- j+1
    
    
    #############################################################
    set.seed(1951) 
    svm.fit <- svm(V1~.,dat.tr.full)
    
    #Predict using SVM regression
    pred.svm = predict(svm.fit, dat.te)
    RMSE[i,j] <- calcRMSE(dat.te[,1], pred.svm)
    predictions.te[,j] <- pred.svm
    j <- j+1
    # plot(dat.te[,1], pred.svm,  main = "RF")
    # abline(0,1)
    

     
    #############################################################
    set.seed(1951) 
    ppr.fit <-
      ppr(dat.tr.full[, -1], dat.tr.full[, 1], nterms = 1, max.terms = 2)
    pred.ppr <-
      ppr.fit %>% predict(dat.te[, -1])              
    RMSE[i,j] <- calcRMSE(dat.te[,1], pred.ppr)
    predictions.te[,j] <- pred.ppr
    j <- j + 1

     #############################################################
    set.seed(1951)    
    brnn.fit <- brnn(as.matrix(dat.tr.full[,-1]), dat.tr.full[, 1])
    pred.brnn <-
      brnn.fit %>% predict(dat.te[, -1])                        
    RMSE[i,j] <- calcRMSE(dat.te[,1], pred.brnn)
    predictions.te[,j] <- pred.brnn
    j <- j + 1
    # plot(dat.tv[,1], pred.brnn,  main = "BRNN")
    # abline(0,1)
    
    #############################################################
    set.seed(1951)     
    gbm.fit <-
      gbm(
        V1 ~ .,
        data = dat.tr.full,
        distribution = "gaussian",
        cv.folds = 5,
        n.trees = 200
      )
    ###prediction
    best.iter <- gbm.perf(gbm.fit, method = "cv")
    
    pred.gbm <-
      gbm.fit %>% predict(dat.te[, -1],  n.trees = best.iter)                          #predict y hat
    RMSE[i,j] <- calcRMSE(dat.te[,1], pred.gbm)
    predictions.te[,j] <- pred.gbm
    j <- j + 1
    # plot(dat.tv[,1], pred.gbm,  main = "GBM")
    # abline(0,1)
    
    RMSE[i,j] <-  calcRMSE(dat.te[,1],   rowMeans(predictions.te[, 1:15]))
 

    colnames(predictions.te) <- c("lasso", "elastic net", "pca", "pls",  "LM+15", "RF", "RF+VS", "Bart", "kPCA","RF+kPCA", "RF+kernel", "svm", "ppr", "brnn", "gbm")
    # pairs(predictions.te)
    
     
   pred.stack.lm <- as.matrix(predictions.te) %*% as.matrix(lm.fit.SG[[i]][[yind]]$coef)
 
   pred.stack.nonneg <- predict(nonneg.fit.SG[[i]][[yind]], predictions.te)
    

   pred.stack.rf <- predict(rf.fit.SG[[i]][[yind]], data = cbind(predictions.te, dat.te[, -1]))$prediction

   pred.stack.lasso <- predict(lasso.fit.SG[[i]][[yind]], predictions.te)
   # plot(pred.stack.nonneg, pred.stack.lm)


   RMSE[i,j+1] <- calcRMSE(dat.te[,1],  pred.stack.lm)
   RMSE[i,j+2] <- calcRMSE(dat.te[,1],  pred.stack.nonneg)
   RMSE[i,j+3] <- calcRMSE(dat.te[,1],  pred.stack.rf)
   RMSE[i,j+4] <- calcRMSE(dat.te[,1],  pred.stack.lasso)
  }
    
  colnames(RMSE) <- c("lasso", "elastic net", "pca", "pls", "LM+15", "RF", "RF+VS", "Bart", "kPCA","RF+kPCA", "RF+kernel", "svm", "ppr", "brnn", "gbm", "ensamble MA",  "ensLM",  "ensNonNeg", "ensRF", "enslasso")
  
  RMSEFULL[[yind]] <- RMSE
  
  RMSE1 <- RMSE %>% as.data.frame() %>% mutate(split = as.factor(1:N)) %>% pivot_longer(1:20,"ALGORITHM")
  # RMSE1 %>% ggplot(aes(x = ALGORITHM, y = value)) + geom_boxplot()
  
  p1 <- RMSE1 %>% group_by(ALGORITHM) %>% mutate(meanv = mean(value)) %>% ggplot(aes(x= ALGORITHM, y = value, color = split, group = split)) +
    geom_point()+ geom_line() +geom_line(aes(x = ALGORITHM, y = meanv), col = 1) + theme(legend.position = "none")+
    ylab("RMSE")+ ggtitle(names(ys_tr)[yind])+
    theme(axis.text.x=element_text(angle = 90, hjust = 0.5, vjust = 0.5))
  
  print(p1)
  
  t1[[yind]] <- RMSE1 %>% group_by(ALGORITHM) %>% summarise(meanv = mean(value), sdv=sd(value)) %>% arrange(desc(meanv))
  
}





save.image("analysisEnsambles2c.Rdata")





```

```{r echo = FALSE}
load("analysisEnsambles2c.Rdata")

RMSEFULLDF <- matrix(0, 850*14, 3) %>%as.data.frame()%>% mutate(trait = rep(names(ys),each = 850))
# glimpse(RMSEFULLDF)

# plot(RMSEFULLDF$Split[1:900])

for(yind in 1:14){  
  a <- (yind-1)*850+1
  b <- yind*850
  d<- (yind-1)*50+1
  e<- yind*50
  RMSE <- RMSEFULL[[yind]]  
  RMSE1 <- RMSE  %>%as.data.frame()%>% mutate(split = d:e)%>% pivot_longer(1:20,"ALGORITHM")
  RMSEFULLDF[a:b,1:3]<- RMSE1
}
names(RMSEFULLDF) <- c("Split", "Algorithm", "RMSE", "Trait")

RMSEFULLDF <- RMSEFULLDF%>% mutate(Split = as.factor(Split), Algorithm = as.factor(Algorithm), Trait = as.factor(Trait))

# data.nest1.aov <- aov(RMSE ~ Algorithm *Trait + Split, RMSEFULLDF)
# anova(data.nest1.aov)
# TukeyHSD(data.nest1.aov)
```

## Fit mixed model to results: 

(Fixed) interaction term is significant. 

```{r echo = TRUE}
data.nest1.lmer <- lmer(RMSE ~ Algorithm*Trait + (1 | Split), RMSEFULLDF, REML = TRUE)

print(summary(data.nest1.lmer)[13])


# plot(allEffects(data.nest1.lmer), axes = list(x=list(rotate = 90)))
```


Using effects package, the error bars are estimate $\pm 1.96 \times$ se.

```{r echo = FALSE}
data.nest1.eff <- as.data.frame(effect("Algorithm:Trait", data.nest1.lmer))
data.nest1.eff <- data.nest1.eff %>% mutate(nonneg = as.numeric(Algorithm=="ensNonNeg"))

# data.nest1.eff$nonneg
# glimpse(data.nest1.eff )
data.nest1.eff %>% ggplot( aes(y = fit, x = Trait, group = Algorithm, colour = Algorithm)) + geom_pointrange(aes(ymin = lower,
    ymax = upper)) + scale_y_continuous("RMSE") + theme(axis.text.x = element_text(angle = 90))+geom_line(aes(alpha = nonneg), show.legend = FALSE)
```


```{r echo = FALSE, fig.height=12, fig.width= 12}

data.nest1.eff %>% ggplot( aes(y = fit, x =  Algorithm)) + geom_errorbar(aes(ymin = lower,
    ymax = upper, colour = nonneg), show.legend = FALSE) + scale_y_continuous("RMSE") + theme(axis.text.x = element_text(angle = 90))+
  facet_wrap(~Trait) 
```


```{r echo = FALSE}
data.nest1.eff <- as.data.frame(Effect("Algorithm", data.nest1.lmer))



data.nest1.eff %>% ggplot( aes(y = fit, x = Algorithm)) + geom_pointrange(aes(ymin = lower,
    ymax = upper)) + scale_y_continuous("RMSE") + theme(axis.text.x = element_text(angle = 90)) + geom_point(size = 3, pch = 1)
# 
# library(sjPlot)
# plot_model(data.nest1.lmer, type = "eff", terms = "Algorithm")
# plot_model(data.nest1.lmer, type = "eff", terms = c("Trait","Algorithm"))
# ?plot_model


# means <- RMSEFULLDF %>% group_by(Algorithm,Trait) %>% summarise(means = mean(RMSE))
# means %>% ggplot(aes(x = Trait, y = means, group = Algorithm, colour = as.factor(Algorithm)))+geom_line()+geom_point(size = 3)
# means <- means %>% mutate(nonneg = as.numeric(Algorithm=="ensNonNeg"))
# means %>% ggplot(aes(x = Trait, y = means, group = Algorithm, colour = Algorithm))+geom_line(aes(alpha = nonneg))+geom_point(size = 3)

# means <- means %>% mutate(nonneg = as.numeric(Algorithm=="ensNonNeg"||Algorithm=="ensamble MA"))
# means %>% ggplot(aes(x = Trait, y = means, group = Algorithm, colour = Algorithm))+geom_line(aes(alpha = nonneg))+geom_point(size = 3, pch = 1)

for(yind in 1:14){
  
    dat <- cbind(ys_tr[,yind], tr)
  
  dat <- as.data.frame(dat)
  p<- dat %>% ggplot(aes(x= V1)) + geom_histogram() + ggtitle(names(ys_tr)[yind])
  print(p)
  
  RMSE <- RMSEFULL[[yind]]  
  RMSE1 <- RMSE  %>%as.data.frame()%>% mutate(split = as.factor(1:N))%>% pivot_longer(1:20,"ALGORITHM")
  # RMSE1 %>% ggplot(aes(x = ALGORITHM, y = value)) + geom_boxplot()
  p1<- RMSE1 %>% group_by(ALGORITHM) %>% mutate(meanv = mean(value, na.rm = TRUE)) %>% ggplot(aes(x= ALGORITHM, y = value, color = split, group = split))+
    geom_point()+ geom_line() +geom_line(aes(x = ALGORITHM, y = meanv), col = 1)+ theme(legend.position = "none")+
    ylab("RMSE")+ ggtitle(names(ys_tr)[yind])+
    theme(axis.text.x=element_text(angle = 90, hjust = 0.5, vjust = 0.5))
  
  print(p1)
  
  LMcoef <- matrix(0,50, 15)
  for (i in 1:50)LMcoef[i,] <- lm.fit.SG[[i]][[yind]]$coefficients
  
  colnames(LMcoef) <- c("lasso", "elastic net", "pca", "pls", "LM+15", "RF", "RF+VS", "Bart", "kPCA","RF+kPCA", "RF+kernel", "svm",  "ppr", "brnn", "gbm")
  
  matplot(t(LMcoef), type = "l", lty = 1, xaxt = "n", col = 9, ylab = "coef", main = "ensemble LM")
  abline(h  = 0)
  axis(1, at = 1 : 15, labels = colnames(LMcoef),las = 2, cex.axis=0.7)
  
   # hist(rowSums(LMcoef))
  
  LMcoef <- matrix(0,50, 15)
  for (i in 1:50)LMcoef[i,] <- coef(nonneg.fit.SG[[i]][[yind]])[2:13]
  colnames(LMcoef) <- c("lasso", "elastic net", "pca", "pls", "LM+15", "RF", "RF+VS", "Bart", "kPCA","RF+kPCA", "RF+kernel", "svm",  "ppr", "brnn", "gbm")
  
  matplot(t(LMcoef), type = "l", lty = 1, xaxt = "n", col = 9, ylab = "coef", main = "ensemble Non Neg")
  abline(h  = 0)
  axis(1, at = 1 : 15, labels = colnames(LMcoef),las = 2, cex.axis=0.7)
  
   # hist(rowSums(LMcoef))
  
   LMcoef <- matrix(0,50, 15)
  for (i in 1:50)LMcoef[i,] <- coef(lasso.fit.SG[[i]][[yind]])[2:13]
  colnames(LMcoef) <- c("lasso", "elastic net", "pca", "pls", "LM+15", "RF", "RF+VS", "Bart", "kPCA","RF+kPCA", "RF+kernel", "svm",  "ppr", "brnn", "gbm")
  
  matplot(t(LMcoef), type = "l", lty = 1, xaxt = "n", col = 9, ylab = "coef", main = "ensemble Lasso")
  abline(h  = 0)
  axis(1, at = 1 : 15, labels = colnames(LMcoef),las = 2, cex.axis=0.7)
  
  
# hist(rowSums(LMcoef))
  
  LMcoef <- matrix(0,50, 15)
  for (i in 1:50)LMcoef[i,] <- rf.fit.SG[[i]][[yind]]$variable.importance[1:15]
  colnames(LMcoef) <- c("lasso", "elastic net", "pca", "pls", "LM+15", "RF", "RF+VS", "Bart", "kPCA","RF+kPCA", "RF+kernel", "svm",  "ppr", "brnn", "gbm")
  
  matplot(t(LMcoef), type = "l", lty = 1, xaxt = "n", col = 9, ylab = "Variable importance", main = "ensemble RF")
  abline(h  = 0)
  axis(1, at = 1 : 15, labels = colnames(LMcoef),las = 2, cex.axis=0.7)
  

  
  # print(t1[yind])
  
  RMSE1 <- RMSE1 %>% mutate(Split = as.factor(split), Algorithm = as.factor(ALGORITHM))
# glimpse(RMSE1)
# b <- aov(value~ split+ ALGORITHM, data = RMSE1)
# anova(b) %>% print()



data.nest1.lmer <- lmer(value ~ Algorithm + (1 | Split), RMSE1, REML = TRUE)

print(summary(data.nest1.lmer)[13])


data.nest1.eff <- as.data.frame(Effect("Algorithm", data.nest1.lmer))



p <- data.nest1.eff %>% ggplot( aes(y = fit, x = Algorithm)) + geom_pointrange(aes(ymin = lower,
    ymax = upper)) + scale_y_continuous("RMSE") + theme(axis.text.x = element_text(angle = 90)) + geom_point(size = 3, pch = 1)+ ggtitle(names(ys_tr)[yind])

print(p)

# TukeyHSD(b)$ALGORITHM[,4][TukeyHSD(b)$ALGORITHM[,4]<0.05] %>% t() %>%t()

# plot(TukeyHSD(b))

}

```

