---
title: "WorkshopPaperKD"
output: pdf_document
editor_options: 
  chunk_output_type: console
---


## Participant KD

In this approach the training set was centered and scaled and the same transformation was applied to the test set. 
No outliers were removed and the full set of 1060 transmittance values was used for training and prediction. Casein micelle size was log transformed. Hierarchical clustering with $1 - $ absolute value of correlation taken as distance matrix and Ward's clustering criterion was applied to the spectra. The resulting dendogram was visualised and the transmittance values were assigned to one of fifteen clusters.

```{r echo=FALSE, fig.height=5, fig.width = 5, message=FALSE, warning=FALSE}
library(tidyverse)
library(ggdendro)
library(dendextend)
library(magrittr)
library(RColorBrewer)
tr <- read.csv("data/tr.csv")
tr <- tr[, -c(1:3)]
a <- apply(tr,2,mean)
b <- apply(tr,2,sd)
tr <- scale(tr,a,b)
getPalette = colorRampPalette(brewer.pal(8, "Dark2"))

D2 <- tr %>% cor() %>% abs()%>% multiply_by(-1) %>% add(1)%>%as.matrix()
dnew <- D2  %>% as.dist()%>% hclust(method = "ward.D2") %>% as.dendrogram() %>% color_branches(k=15, col = getPalette(15))

dnew%>% set("labels_col", value = c(0)) %>% plot()


```

The dataset was initially split into 50 random splits of training (50\%) and validation (50\%) sets to select the optimal algorithm for regression.
All analysis was done in R \cite{R}. 
Following algorithms were used for regression:

* lasso from (glmnet library \cite{glmnet})
* random forest (ranger \cite{ranger}) with and without regularization (\cite{regrf})
* linear regression with 6 PCs with highest eigenvalues
* linear regression with 6 KPCs with highest eigenvalues using Gaussian kernel (\cite{kernlab})
* random forest with 6 KPCs with highest eigenvalues using Gaussian kernel
* random forest with a Gaussian kernel
* support vector machine for regression (library e1071 \cite{e1071})
* partial least squares (pls) with 3, 4, and 5 components \cite{pls}
* linear regression with 15 wavelengths selected by a) clustering the wavelengths into 15 clusters and selecting the wavelength from each cluster that has the highest correlation with the response variable
* Bayesian additive regression trees (BART) \cite{bart} from bartMachine library \cite{bartMachine}

The algorithms were tuned using further cross-validation of the training sets. 

Finally, an ensemble model was used: averages taken over predictions in test sets from the models described above.
The predictive ability of the algorithms was evaluated based on RMSE. The best scoring algorithms over the 50 random splits for all three traits was the ensamble model. The predictions of this model trained over the entire training set  were submitted to the competition. 



